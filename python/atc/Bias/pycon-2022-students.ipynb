{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDxtnoIr7mIN"
   },
   "source": [
    "# PyCon 2022 Tutorial <br>_Assessing and mitgating unfairness in AI Systems_\n",
    "\n",
    "---\n",
    "This tutorial _Assessing and mitigating unfairness in AI systems_ is licensed under\n",
    "[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/). \n",
    "\n",
    "It is adapted from \n",
    "_Fairness in AI systems: From social context to practice using Fairlearn_ presented by Manojit Nandi, Miroslav Dudík, Triveni Gandhi, Lisa Ibañez, Adrin Jalali, Michael Madaio, Hanna Wallach, Hilde Weerts at _SciPy 2021_. \n",
    "*GitHub URL*: https://github.com/fairlearn/talks/tree/main/2021_scipy_tutorial\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sch9KDWg7SL8"
   },
   "source": [
    "Fairness in AI systems is an interdisciplinary field of research and practice that aims to understand and address some of the negative impacts of AI systems on society. In this tutorial, we will walk through the process of assessing and mitigating fairness-related harms in the context of the U.S. health care system. This tutorial will consist of a mix of instructional content and hands-on demonstrations using Jupyter notebooks. Participants will use the Fairlearn library to assess ML models for performance disparities across different racial groups and mitigate those disparities using a variety of algorithmic techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fboVvqvVvpKz"
   },
   "source": [
    "# **Prepare environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0p461hmgrmz"
   },
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this notebook through _Google Colab_, run the below cell to install the necessary packages. Please restart the runtime afterwards.\n",
    "\n",
    "If you are running the notebook in a local environment, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "w_uVNMHUdb2w",
    "outputId": "8ef912ea-10cf-47ef-c85d-574a044283e3"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade fairlearn==0.7.0\n",
    "!pip install --upgrade scikit-learn>=1.0.0\n",
    "!pip install --upgrade seaborn\n",
    "!pip install --use-deprecated=legacy-resolver model-card-toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVtpwFGJLcgU"
   },
   "source": [
    "## Import and set up packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEwLsWyTLgJn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.float_format\", \"{:.3f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBOSsyc48MK0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhbFzU6GLgW0"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    plot_roc_curve)\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcilMnWhLgaT"
   },
   "outputs": [],
   "source": [
    "from fairlearn.metrics import (\n",
    "    MetricFrame,\n",
    "    true_positive_rate,\n",
    "    false_positive_rate,\n",
    "    false_negative_rate,\n",
    "    selection_rate,\n",
    "    count,\n",
    "    false_negative_rate_difference\n",
    ")\n",
    "\n",
    "from fairlearn.postprocessing import ThresholdOptimizer, plot_threshold_optimizer\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, TruePositiveRateParity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4ANMj1M8k0h"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Card Toolkit works in Google Colab, but it does not work on all local environments\n",
    "# that we tested. If the import fails, define a dummy function in place of the function\n",
    "# for saving figures into images in a model card.\n",
    "\n",
    "try:\n",
    "    from model_card_toolkit import ModelCardToolkit\n",
    "    from model_card_toolkit.utils.graphics import figure_to_base64str\n",
    "    import model_card_toolkit as mctlib\n",
    "    model_card_imported = True\n",
    "except Exception:\n",
    "    model_card_imported = False\n",
    "    def figure_to_base64str(*args):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gZTFumIxCfK"
   },
   "source": [
    "# **Introduction of Fairlearn and other tutorial resources**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KhsuCB-imFk"
   },
   "source": [
    "This tutorial builds on the following open source projects:\n",
    "\n",
    "* **machine learning and data processing**: _scikit-learn_, _pandas_, _numpy_\n",
    "* **plotting**: _seaborn_, _matplotlib_\n",
    "* **AI fairness**: _Fairlearn_, _Model Card Toolkit_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORfTAukTuEPN"
   },
   "source": [
    "### [Fairlearn](https://fairlearn.org)\n",
    "\n",
    "Fairlearn is an open-source, community-driven project to help data scientists improve fairness of AI systems. It includes:\n",
    "\n",
    "* A Python library for fairness assessment and improvement (fairness metrics, mitigation algorithms, plotting, etc.)\n",
    "\n",
    "* Educational resources covering organizational and technical processes for unfairness mitigation (user guide, case studies, Jupyter notebooks, etc.)\n",
    "\n",
    "The project was started in 2018 at Microsoft Research. In 2021 it adopted neutral governance structure and since then it is completely community-driven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AE3bFABst9ze"
   },
   "source": [
    "### [Model Card Toolkit](https://github.com/tensorflow/model-card-toolkit)\n",
    "\n",
    "The Model Card Toolkit (MCT) streamlines and automates generation of _model cards_, machine learning documents that provide context and transparency into a model's development and performance. It was released by Google in 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OoPTetsDv-v"
   },
   "source": [
    "# **Overview of fairness in AI systems**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt9n43o6DPbg"
   },
   "source": [
    "If you have read anything in the news over the past decade, you'll know we are \"living in the age of AI\". It seems as if every week, there is a story about some new advancement in applying AI and machine learning to real-world problems. However, with all these opportunities comes certain challenges. In particular, challenges that have received a lot of attention in the media and have really highlighted how important it is to get AI right - to make sure that AI does not discriminate or further disadvantage already disadvantaged groups of people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness-related harms in the media\n",
    "\n",
    "Some of these media stories have involved high-stakes decisions where AI systems are used to allocate opportunities or resources in ways that can have signifiant negative impacts on people's lives. For example, a [ProPublica investigation](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm) showed that COMPAS, a widely used recidivism risk assessment tool, incorrectly scored Black defendants as high-risk more often than white defendants, while incorrectly scoring white defendants as low-risk more often than Black defendants.\n",
    "\n",
    "![ProPublica](https://lh5.googleusercontent.com/5yLvtiJz85NQ2RJJ7Qzw0yOCnB2M03324HX1vcTlUJUwbFhgizVJAHsP-dqF3f9TXLhVIt9Mn6qeQkqZgTzdSjj1B9zcLwk_qdyjBL4tOshzgZoaFe8Z32vR78B8Q8EmhLz4LSlu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other stories have involved much more mundane AI systems. Researchers at Princeton University found that translating the sentences \"He is a nurse.\" and \"She is a doctor.\" in Turkish, a genderless language, and then back into English yielded the stereotypical (and in this case, incorrect) translations \"She is a nurse\" and \"He is a doctor\"\n",
    "\n",
    "![Gendered translations](https://www.i-programmer.info/images/stories/News/2017/apr/B/genderbiasgt.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A sociotechnical challenge\n",
    "\n",
    "Even though we can often spot fairness-related harms when we see them, there's no one-size-fits-all definition of fairness that applies to all AI systems in all contexts. Fairness in AI is a fundamentally *sociotechnical* challenge, so it cannot be approached from purely social or technical perspectives. This can make it a particularly daunting landscape to navigate. But, although there are few easy answers, there are a variety of emerging strategies and tools for assessing and mitigating fairness-related harms, as well as a deepening understand of the challenge throughout society."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid using the term \"bias\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have ever read anything about fairness in AI systems, you have probably heard the term \"bias\" used a lot - often as a catch-all way to describe any unfair behavior of an AI system and any possible causes of that unfair behavior.\n",
    "\n",
    "However, we are going to avoid using the word \"bias\" whenever possible, and we recommend you do the same. This is because the word \"bias\" is ambiguous and means very different things to different communities (e.g. statistical bias versus societal biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness-related harms\n",
    "\n",
    "Instead, we will focus on the fairness-related impacts that AI systems can have on people -- that is, specific types of fairness-related harms. By focusing on fairness-related harms, we can better disentangle who might be harmed by a system and in what ways. This approach also enables us to choose better different assessment and mitigation strategies.\n",
    "\n",
    "One point that we want to emphasize is that is not possible to fully \"debias\" an AI system or to guarantee its fairness. This is because there are so many reasons why AI systems can cause fairness-related harms. As a result, we also don't recommend using words like *debiasing* and *unbiased*, which can set up unrealistic expectations. Instead, we recommend talking about assessing and mitigating fairness-related harms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Algorithmic Harms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, there are five main types of fairness-related harms that can be caused by an AI system: \n",
    "1. Allocation\n",
    "2. Quality of service\n",
    "3. Stereotyping\n",
    "4. Demeaning\n",
    "5. Over- or under-representation.\n",
    "\n",
    "To explain each of these five types, we will run through some illustrative examples, starting with the most well-known type – allocation. One important note is that these types are not mutually exclusive. It’s possible for a single AI system to exhibit more than one type of harm. And in fact, many of the examples below could have been used to illustrate one or more of the other types as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harms of Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first type of harm is **harms of allocation**: Does the system allocate opportunities or resources in a way that negatively impacts a group of people? \n",
    "\n",
    "Many media stories focus on high-stakes decisions where AI systems have been used to allocate opportunites or resources in ways that have negative impact on people's lives. For example, Amazon abandoned its automated resume hiring system after finding [the system amplified the existing gender imbalance in the tech industry by withholding employment opportunities from women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Amazon scraps AI tool](https://th.bing.com/th/id/OIP.Z-1GRh2pPaC6WwwZk7tbRAHaFP?pid=ImgDet&rs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality of service harms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second type of harm is **quality of service harms**. These harms focus on whether an AI system works as well for one person as it does for another, even if no opportunities or resources are extended or withheld. In a famous example, researchers ([Buolamwini and Gebru, 2018](http://gendershades.org/)) found three commerical facial recognition systems had higher error rates on faces of women with darker skin tones compared to men with lighter skin tones. \n",
    "\n",
    "Like accessibility issues, *quality of service harms* can raise questions about respect, dignity, and personhood. Imagine how someone may feel if a voice recognition system fails to recognize her voice, but easily recognizes the voices of her peers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gender Shades](https://th.bing.com/th/id/OIP.2iwtwtDThcg3iNGpFhT6GwHaFP?w=251&h=180&c=7&r=0&o=5&dpr=1.5&pid=1.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stereotyping harms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third type of harm is **stereotyping harms**. These harms occur when a system promotes or reinforces harmful or negative stereotypes about a group of people. Latanya Sweeny [(Sweeny, 2013)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2208240) showed online advertisements were more likely suggest people with Black-sounding names had been arrested, thereby reinforcing negative stereotypes about Black criminality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Latanya Sweeney](https://i.huffpost.com/gen/975291/thumbs/o-ONLINE-AD-RACISM-570.jpg?5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demeaning harms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth type of harm is when an AI system is actively **demeaning**. When it first launched, Google Photos infamously labeled a picture of Black teens as \"gorillas\". This mislabeling is harmful, not just because it is incorrect, but because it specifically applied a label that has historically been used to demean Black individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Google Photos incident](https://www.techlicious.com/images/cameras/gorillas-tag-google-photos-controversy-510px.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harms of over-and-under representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fifth type of algorithmic harm is **harms of over-and-under representation**: Does the system over-represent or under-represent certain groups of people, or possibly erase some groups of people entirely?\n",
    "\n",
    "Researchers at the University of Washington examined the image search results for professions with an equal or higher number of men compared to women, such as engineering. These search results were heavily skewed towards men; far more than reality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CEO bias](https://i.huffpost.com/gen/2820944/original.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harms are not mutually exclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, the five main types of fairness-related harms that can be caused by anAI system are *allocation*, *quality of service*, *stereotyping*, *demeaning*, and *over and under-representation*. As mentioned earlier, these harms are neither mutually exclusive. An single AI system can exhibit multiple types of fairness-related harms and can exhibit different harms for different groups of people. For example, the automated system for screening resumes display not just *allocation harms* as well as *quality of service*, *stereotyping*, and *over-and-under representation* harms.\n",
    "\n",
    "It's also import to keep in mind that fairness-related harms can have varying severity. Unfairly denying someone bail is more severe harm than labeling an image of a female doctor as a nurse, but it's also important to remember that even relatively \"non-severe\" harms can make people feel alienated or singled out, and their cumulative impact can be extremely burdensome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                                                                               | Allocation | Quality-of-Service | Stereotyping | Demeaning | Over-and-Under Representation |   |   |   |   |\n",
    "|-------------------------------------------------------------------------------|------------|--------------------|--------------|-----------|-------------------------------|---|---|---|---|\n",
    "| Automated system does not rank women as highly as men for technical jobs      | X          | X                  | X            |           | X                             |   |   |   |   |\n",
    "| Gender classification system has higher error rate for women with darker skin |            | X                  |              |           |                               |   |   |   |   |\n",
    "| Machine translation system exhibits male/female gender stereotypes            |            |                    | X            |           | X                             |   |   |   |   |\n",
    "| Photo management system labels images of Black teens as \"gorillas\"            |            | X                  |              | X         |                               |   |   |   |   |\n",
    "| Image search for \"CEO\" yields only photos of white men on the first page      |            |                    | X            |           | X                             |   |   |   |   |\n",
    "|                                                                               |            |                    |             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_j1vtg6TD7Fi"
   },
   "source": [
    "# **Introduction to the health care scenario**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUkG_zdEylGU"
   },
   "source": [
    "Our scenario builds on previous research that highlighted racial disparities in how health care resources are allocated in the U.S. ([Obermeyer et al., 2019](https://science.sciencemag.org/content/366/6464/447.full)).\n",
    "Motivated by that work, in this tutorial we consider an automated system for recommending patients for _high-risk care management_ programs, which are described by Obermeyer et al. 2019 as follows:\n",
    "\n",
    "> These programs seek to improve the care of patients with complex health needs by providing additional resources, including greater attention from trained providers, to help ensure that care is well coordinated. Most health systems use these programs as the cornerstone of population health management efforts, and they are widely considered effective at improving outcomes and satisfaction while reducing costs. [...] Because the programs are themselves expensive—with costs going toward teams of dedicated nurses, extra primary care appointment slots, and other scarce resources—**health systems rely extensively on algorithms to identify patients who will benefit the most.**\n",
    "\n",
    "**Convenience restriction**\n",
    "\n",
    "* In practice, the modeling of health needs would use large data sets covering a wide range of diagnoses. In this tutorial, we will work with a [publicly available clinical dataset](https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008) that focuses on _diabetic patients only_ ([Strack et al., 2014](https://www.hindawi.com/journals/bmri/2014/781670/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-j4KN95wLLS"
   },
   "source": [
    "## Dataset and task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOwrRsB7wEeM"
   },
   "source": [
    "We will be working with a clincial dataset of hospital re-admissions over a ten-year period (1998-2008) for diabetic patients across 130 different hospitals in the US. Each record represents the hospital admission records for a patient diagnosed with diabetes whose stay lasted one to fourteen days.\n",
    "\n",
    "The features describing each encounter include demographics, diagnoses, diabetic medications, number of visits in the year preceding the encounter, and payer information, as well as whether the patient was readmitted after release, and whether the readmission occurred within 30 days of the release.\n",
    "\n",
    "We would like to develop a classification model, which decides whether the patients should be suggested to their primary care physicians for an enrollment into the high-risk care management program. The positive prediction will mean recommendation into the care program.\n",
    "\n",
    "**Decision point: Task definition**\n",
    "\n",
    "* A hospital **readmission within 30 days** can be viewed as a proxy that the patients needed more assistance at the release time, so it will be the label we wish to predict.\n",
    "\n",
    "* Because of the class imbalance, we will be measuring our performance via **balanced accuracy**. Another key performance consideration is how many patients are recommended for care, metric we refer to as **selection rate**.\n",
    "\n",
    "Ideally, health care professionals would be involved in both designing and using the model, including formalizing the task definition. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BE26iXWwUqr"
   },
   "source": [
    "## Fairness considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZUcQVZYyRvz"
   },
   "source": [
    "* _Which groups are most likely to be disproportionately negatively affected?_ Previous work suggests that groups with different race and ethnicity can be differently affected.\n",
    "\n",
    "* _What are the harms?_ The key harms here are allocation harms. In particular, false negatives, i.e., don't recommend somebody who will be readmitted.\n",
    "\n",
    "* _How should we measure those harms?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kD6G-yF1Tcf"
   },
   "source": [
    "In the remainder of the tutorial we will:\n",
    "* First examine the dataset and our choice of label with an eye towards a variety of fairness issues.\n",
    "* Then train a logistic regression model and assess its performance as well as fairness.\n",
    "* Finally, look at two unfairness mitigation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7tpRumX_4Nh"
   },
   "source": [
    "# Task definition and dataset characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iJGhfCgPiy-"
   },
   "source": [
    "Two critical decisions when desiging an AI system are\n",
    "1. how we define the machine learning task\n",
    "2. what dataset we use to train our models\n",
    "\n",
    "These choices are often intertwined, because the dataset is often a convenience dataset, based on availability, which leads to a specific choice of label and performance metric (that's also the case in our scenario).\n",
    "\n",
    "In this part of the tutorial, we first load the dataset, and then we examine it for a variety of fairness issues:\n",
    "1. sample sizes of different demographic groups, and in particular different racial groups\n",
    "2. appropriateness of our choice of label (readmission within 30 days)\n",
    "3. representativeness/informativeness of different features for different groups\n",
    "\n",
    "Besides dataset characteristics, one additional aspect of dataset fairness is whether the data was collected in a manner that respects the autonomy of individuals in the dataset.\n",
    "\n",
    "The dataset characteristics can be systematically documented through the **datasheets** practice. We will touch on this later on. By documenting our understanding of the dataset, we communicate any concerns we have about the data and highlight downstream issues that may arise during the model training, evaluation and deployment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-ABnntZT8Fn"
   },
   "source": [
    "## Load the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvyHqcLIT8Fo"
   },
   "source": [
    "We next load the dataset and review the meaning of its columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkfwniFQT8Fp"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/fairlearn/talks/main/2021_scipy_tutorial/data/diabetic_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "mIFN96kiT8Fq",
    "outputId": "9dbf587e-5f51-4dc4-877a-3ffb9a7374a2"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXQNItgRT8Fu"
   },
   "source": [
    "The columns contain mostly boolean and categorical data (including age and various test results), with just the following exceptions: `time_in_hospital`, `num_lab_procedures`, `num_procedures`, `num_medications`, `number_diagnoses`.\n",
    "\n",
    "\n",
    "|features| description|\n",
    "|---|---|\n",
    "| race, gender, age | demographic features |\n",
    "| medicare, medicaid | insurance information |\n",
    "| admission_source_id | emergency, referral, or other |\n",
    "| had_emergency, had_inpatient_days,<br>had_outpatient_days | hospital visits in prior year |\n",
    "| medical_specialty | admitting physician's specialty |\n",
    "| time_in_hospital, num_lab_procedures,<br>num_procedures, num_medications,<br>primary_diagnosis, number_diagnoses,<br>max_glu_serum, A1Cresult, insulin<br>change, diabetesMed | description of the hospital visit<br> |\n",
    "| discharge_disposition_id | discharched to home or not |\n",
    "| readmitted, readmit_binary,<br>readmit_30_days | readmission information |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 738
    },
    "id": "gPtaDcfHT8Fv",
    "outputId": "42347f1f-d3c8-4221-f98d-fdeaa25ef4fd"
   },
   "outputs": [],
   "source": [
    "# Show the values of all binary and categorical features\n",
    "categorical_values = {}\n",
    "for col in df:\n",
    "  if col not in {'time_in_hospital', 'num_lab_procedures',\n",
    "                 'num_procedures', 'num_medications', 'number_diagnoses'}:\n",
    "    categorical_values[col] = pd.Series(df[col].value_counts().index.values)\n",
    "categorical_values_df = pd.DataFrame(categorical_values).fillna('')\n",
    "categorical_values_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8loEiuFSWb8A"
   },
   "source": [
    "We mark all categorical features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4y1FRMdWduE"
   },
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"age\",\n",
    "    \"discharge_disposition_id\",\n",
    "    \"admission_source_id\",\n",
    "    \"medical_specialty\",\n",
    "    \"primary_diagnosis\",\n",
    "    \"max_glu_serum\",\n",
    "    \"A1Cresult\",\n",
    "    \"insulin\",\n",
    "    \"change\",\n",
    "    \"diabetesMed\",\n",
    "    \"readmitted\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qyo3GQ4RWduG"
   },
   "outputs": [],
   "source": [
    "for col_name in categorical_features:\n",
    "  df[col_name] = df[col_name].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LTax67Em4q8"
   },
   "source": [
    "## Group sample sizes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft28kXKHm4q9"
   },
   "source": [
    "From the perspective of fairness assessment, a key data characteristic is the sample size of groups with respect to which we conduct fairness assessment.\n",
    "\n",
    "Small sample sizes have two implications:\n",
    "\n",
    "* **assessment**: the impacts of the AI system on smaller groups are harder to assess, because due to fewer data points we have a much larger uncertainty (error bars) in our estimates\n",
    "\n",
    "* **model training**: fewer training data points mean that our model fails to appropriately capture any data patterns specific to smaller groups, which means that its predictive performance on these groups could be worse\n",
    "\n",
    "Let's examine the sample sizes of the groups according to `race`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEGJjCrOm4q9",
    "outputId": "a7430662-00d7-405b-b9af-1dcaf8fc5c28"
   },
   "outputs": [],
   "source": [
    "df[\"race\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "HznSDLWEm4q-",
    "outputId": "009b8aea-9ae4-4810-eb13-67ac8efae163"
   },
   "outputs": [],
   "source": [
    "df[\"race\"].value_counts().plot(kind='bar', rot=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkI6KA3rm4q-"
   },
   "source": [
    "Normalized as frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qE2UoDU3m4q-",
    "outputId": "c23f07b1-493d-4df2-db5a-06bc921eefde"
   },
   "outputs": [],
   "source": [
    "df[\"race\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEUrg6J3m4q_"
   },
   "source": [
    "In our dataset, our patients are predominantly *Caucasian* (75%). The next largest racial group is *AfricanAmerican*, making up 19% of the patients. The remaining race categories (including *Unknown*) compose only 6% of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIdDhgtAm4rA"
   },
   "source": [
    "We also examine the dataset composition by `gender`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0h26CH50m4rA",
    "outputId": "20d290a6-c8a7-4d21-c676-2a6f552bc3a6"
   },
   "outputs": [],
   "source": [
    "df[\"gender\"].value_counts() # counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UTbI5v48m4rA",
    "outputId": "e69992ee-8b33-4aef-a903-b51eb63094e8"
   },
   "outputs": [],
   "source": [
    "df[\"gender\"].value_counts(normalize=True) # frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3xBANdpm4rA"
   },
   "source": [
    "Gender is in our case effectively binary (and we have no further information how it was operationalized), with both *Female* represented at 54% and *Male* represented at 46%. There are only 3 samples annotated as *Unknown/Invalid*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyLBXpKWm4rA"
   },
   "source": [
    "### Decision point: How do we address smaller group sizes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wT-BPQ2Gm4rB"
   },
   "source": [
    "When the data set lacks coverage of certain groups, it means that we will not be able to reliably assess any fairness-related issues. There are three interventions (which could be carried out in a combination):\n",
    "\n",
    "* **collect more data**: collect more data for groups with fewer samples\n",
    "* **buckets**: merge some of the groups\n",
    "* **drop small groups**\n",
    "\n",
    "The choice of strategy depends on our existing understanding of which groups are at the greatest risk of a harm. In particular, pooling the groups with widely different risks could mask the extent of harms. We generally caution against dropping small groups as this leads to the representational harm of erasure.\n",
    "\n",
    "If any groups are merged or dropped, these decisions should be annotated / explained (in the datasheet, which we discuss below).\n",
    "\n",
    "In our case, we will:\n",
    "\n",
    "* merge the three smallest race groups *Asian*, *Hispanic*, *Other* (similar to [Strack et al., 2014](https://www.hindawi.com/journals/bmri/2014/781670/)), but also retain the original groups for auxiliary assessments\n",
    "\n",
    "* drop the gender group *Unknown/Invalid*, because the sample size is so small that no meaningful fairness assessment is possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBN_fIhpm4rB"
   },
   "outputs": [],
   "source": [
    "# drop gender group Unknown/Invalid\n",
    "df = df.query(\"gender != 'Unknown/Invalid'\")\n",
    "\n",
    "# retain the original race as race_all, and merge Asian+Hispanic+Other \n",
    "df[\"race_all\"] = df[\"race\"]\n",
    "df[\"race\"] = df[\"race\"].replace({\"Asian\": \"Other\", \"Hispanic\": \"Other\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_Sb8ISAnRQF"
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ns1Tr9wLm4rB"
   },
   "source": [
    "Please examine the distribution of the `age` feature in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDiLp-cDoWGv"
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOn4j1o8m4rB",
    "outputId": "577b1636-57f5-4ab1-903a-c6a42f8193ba"
   },
   "outputs": [],
   "source": [
    "df[\"age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "ns7RxP5um4rB",
    "outputId": "81364da1-1f2e-4053-ced1-c93394ba125e"
   },
   "outputs": [],
   "source": [
    "df[\"age\"].value_counts().plot(kind='bar', rot=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAXa7MoQnjq4"
   },
   "source": [
    "As we might expect, most patients admitted into the hospital in our data set belong to the *Over 60 years* category. Although we will not be assessing for age-based fairness-related harms in this tutorial, we will want to document the age imbalance in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK0LbF_sylTU"
   },
   "source": [
    "## Examining the choice of label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtgLr5Cs_YhF"
   },
   "source": [
    "Next we dive into the question of whether our choice of label (readmission within 30 days) aligns with our goal (identify patients that would benefit from the care management program).\n",
    "\n",
    "A framework particularly suited for this analysis is called _measurement modeling_ (see, e.g., [Jacobs and Wallach, 2021](https://arxiv.org/abs/1912.05511)). The goal of measurement modeling is to describe the relationship between what we care about and what we can measure. The thing that we care about is referred to as the _construct_ and what we can observe is referred to as the _measurement_. In our case:\n",
    "* **construct** = greatest benefit from the care management program\n",
    "* **measurement** = readmission within 30 days (in the absence of such program)\n",
    "\n",
    "In our case, the **measurement** coincides with the **classification label**.\n",
    "\n",
    "The act of _operationalizing_ the construct via a specific measurement corresponds to making certain assumptions. In our case, we are making the following assumption: **the greatest benefit from the care management program would go to patients that are** (in the absence of such a program) **most likely to be readmitted into the hospital within 30 days.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA19isovvCew"
   },
   "source": [
    "### How can we check whether our assumptions apply?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oY95_kOFO6Wb"
   },
   "source": [
    "In the terminology of measurement modeling, how do we establish _construct validity_? Following, [Jacobs and Wallach, 2021](https://arxiv.org/abs/1912.05511),\n",
    "\n",
    "> Establishing construct\n",
    "validity means demonstrating, in a variety of ways, that the measurements obtained from measurement model are both meaningful\n",
    "and useful:\n",
    "> * Does the operationalization capture all relevant aspects\n",
    "of the construct purported to be measured?\n",
    "> * Do the measurements\n",
    "look plausible?\n",
    "> * Do they correlate with other measurements of the\n",
    "same construct? Or do they vary in ways that suggest that the\n",
    "operationalization may be inadvertently capturing aspects of other\n",
    "constructs?\n",
    "> * Are the measurements predictive of measurements of\n",
    "any relevant observable properties (and other unobservable theoretical constructs) thought to be related to the construct, but not incorporated into the operationalization?\n",
    "\n",
    "We focus on one aspect of construct validity, called _predictive validity_, which refers to the extent\n",
    "to which the measurements obtained from a measurement model\n",
    "are predictive of measurements of any relevant observable properties \n",
    "related to the construct purported to be measured, but not incorporated into the operationalization.\n",
    "\n",
    "The predictions do not need to be chronological, meaning that we do not necessarily need to be predicting future from the past. Also, the predictions do not need to be causal (going from causes to effects). We just need to ensure that the predicted property is not part of the measurement whose validity we're checking.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IBUG3Sa96LX"
   },
   "source": [
    "### Predictive validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVrLpuwy98uG"
   },
   "source": [
    "We would like to show that our measurement `readmit_30_days` is correlated with patient characteristics that are related to our construct \"benefiting from care management\". One such characteristic is the general patient health, where we expect that patients that are less healthy are more likely to benefit from care management.\n",
    "\n",
    "While our data does not contain full health records that would enable us to holistically measure general patient health, the data does contain two relevant features: `had_emergency` and `had_inpatient_days`, which indicate whether the patient spent any days in the emergency room or in the hospital (but non-emergency) in the preceding year.\n",
    "\n",
    "To establish predictive validity, we would like to show that our measurement `readmit_30_days` is predictive of these two observable characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQWxJEN-M6VD"
   },
   "source": [
    "First, let's check the rate at which the patients with different `readmit_30_days` labels were readmitted in the previous year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "3t11OgTgZfV8",
    "outputId": "d2467bd3-0148-4373-cd41-0b99d9f54088"
   },
   "outputs": [],
   "source": [
    "sns.pointplot(y=\"had_emergency\", x=\"readmit_30_days\",\n",
    "              data=df, ci=95, join=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ptl-tHkDf_GJ"
   },
   "source": [
    "The plot shows that indeed patients with `readmit_30_days=0` have a lower rate of emergency visits in the prior year, whereas patients with `readmit_30_days=1` have a larger rate. (The vertical lines indicate 95% confidence intervals obtained via boostrapping.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07GU8IGIY9KC"
   },
   "source": [
    "We see a similar pattern when `readmit_30_days` is used to predict the rate of (non-emergency) hospital visits in the previous year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "GuPPVpXrO5uE",
    "outputId": "dcc740aa-0805-4ef9-9c77-d64fe1b0111a"
   },
   "outputs": [],
   "source": [
    "sns.pointplot(y=\"had_inpatient_days\", x=\"readmit_30_days\",\n",
    "              data=df, join=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wN8NU8QkRMqM"
   },
   "source": [
    "Now let's take a look whether the predictiveness is similar across different race groups. First, let's check how well `readmit_30_days` predicts `had_emergency`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "tgLhUlZzOvHC",
    "outputId": "59590db9-f5c7-44a0-85d2-bfcf79eab25e"
   },
   "outputs": [],
   "source": [
    "# Visualize predictiveness using a categorical pointplot\n",
    "sns.catplot(y=\"had_emergency\", x=\"readmit_30_days\", hue=\"race\", data=df,\n",
    "            kind=\"point\", ci=95, dodge=True, join=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ7zH3Wn416g"
   },
   "source": [
    "The patients in the group *Unknown* have a substantially lower rate of emergency visits in the prior year, regardless of whether they are readmitted in 30 days. The readmission is still positively correlated with `had_emergency`, but note the large error bars (due to small sample sizes).\n",
    "\n",
    "We also see that the group with feature value *AfricanAmerican* has a higher rate of emergency visits compared with other groups. However, generally the groups *Caucasian*, *AfricanAmerican* and *Other* follow similar dependence patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G13abbdS7oM-"
   },
   "source": [
    "We see a similar pattern when `readmit_30_days` is used to predict the rate of (non-emergency) hospital visits in the previous year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "Kx52yWYNQuSH",
    "outputId": "d0acd516-57d5-4910-feaa-f3b1c1176900"
   },
   "outputs": [],
   "source": [
    "sns.catplot(y=\"had_inpatient_days\", x=\"readmit_30_days\", hue=\"race\", data=df,\n",
    "            kind=\"point\", ci=95, dodge=True, join=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrCelj5_hR6a"
   },
   "source": [
    "Again, for *Unknown* the rate of (non-emergency) hospital visits in the previous year is lower than for other groups.In all groups there is a strong positive correlation between `readmit_30_days` and `had_inpatient_days`.\n",
    "\n",
    "In all cases, we see that readmission in 30 days is predictive of our two measurements of general patient health.\n",
    "\n",
    "The analysis is also surfacing the fact that patients with the value of race *Unknown* have fewer hospital visits in the preceding year (both emergency and non-emergency) than other groups. In practice, this would be a good reason to reach out to health professionals to investigate this patient cohort, to make sure that we understand why there is the systematic difference.\n",
    "\n",
    "Note that we have only investigated _predictive validity_, but there are other important aspects of construct validity which we may want to establish (see [Jacobs and Wallach, 2021](https://arxiv.org/abs/1912.05511))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_4CZt-UBaOQ"
   },
   "source": [
    "<a name=\"exercise-predictive-validity\"></a>\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rW9ktRlqBhZA"
   },
   "source": [
    "Check the predictive validity with respect to `gender` and `age`. Do you see any differences? Can you form a hypothesis why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "VLom8GFbUfqR",
    "outputId": "91418775-b94d-4b3e-dba4-1305ef908a29"
   },
   "outputs": [],
   "source": [
    "# Check for predictive validity by gender\n",
    "sns.catplot(y=\"had_inpatient_days\",x=\"readmit_30_days\",hue=\"gender\", data=df,\n",
    "            kind=\"point\", ci=95, dodge=True, join=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "7PdFAu3lT-pD",
    "outputId": "7c3b6008-fdff-46dd-f749-ca0198a5193b"
   },
   "outputs": [],
   "source": [
    "# Check for predictive validity by age\n",
    "sns.catplot(y=\"had_inpatient_days\", x=\"readmit_30_days\", hue=\"age\", data=df,\n",
    "            kind=\"point\", ci=95, dodge=True, join=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i9KdmTWKUJZ"
   },
   "source": [
    "## Label imbalance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViGqA5VTGrEo"
   },
   "source": [
    "Now that we have established the validity of our label, we will check frequency of its values in our data. The frequency of different labels is an important descriptive characteristic in classification settings for several reasons:\n",
    "\n",
    "* some classification algorithms and performance measures might not work well with data sets with extreme class imbalance\n",
    "* in binary classification settings, our ability to evaluate error is often driven by the size of the smaller of the two classes (again, the smaller the sample the larger the uncertainty in estimates)\n",
    "* label imbalance may exacerbate the problems due to smaller group sizes in fairness assessment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cos3--59EiZt"
   },
   "source": [
    "Let's check how many samples in our data are labeled as positive and how many as negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eR5ULLYGE4UK",
    "outputId": "02f2e533-2a7f-44af-e57f-c24c7b8b4d4d"
   },
   "outputs": [],
   "source": [
    "df[\"readmit_30_days\"].value_counts() # counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UWBbg4Cz90t",
    "outputId": "a0e1e14b-47f9-4f49-da41-ba53a7a63082"
   },
   "outputs": [],
   "source": [
    "df[\"readmit_30_days\"].value_counts(normalize=True) # frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-_K_KHXz9MR"
   },
   "source": [
    "As we can see, the target label is heavily skewed towards the patients not being readmitted within 30 days. In our dataset, only 11% of patients were readmitted within 30 days.\n",
    "\n",
    "Since there are fewer positive examples, we expect that we will have a much larger uncertainty (error bars) in our estimates of *false negative rates* (FNR), compared with *false positive rates* (FPR). This means that there will be larger differences between training FNR and test FNR, even if there is no overfitting, simply because of the smaller sample sizes. \n",
    "\n",
    "Our target metric is *balanced error rate*, which is the average of FPR and FNR. The value of this metric is robust to different frequencies of positives and negatives. However, since half of the metric is contributed by FNR, we expect the uncertainty in balanced error values to behave similarly to the uncertainty of FNR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkopoyE3GQ8g"
   },
   "source": [
    "Now, let's examine how much the label frequencies vary within each group defined by `race`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "tcH3Mxwmm4rC",
    "outputId": "75641d75-670d-4a81-a354-855f80c38613"
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=\"readmit_30_days\", y=\"race\", data=df, ci=95);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox06aTMmm4rB"
   },
   "source": [
    "We see the rate of *30-day readmission* is similar for the *AfricanAmerican* and *Caucasian* groups, but appears smaller for *Other* and smallest for *Unknown* (this is consistent with an overall lower rate of hospital visits in the prior year). The smaller sample size of the *Other* and *Unknown* groups mean that there is more uncertainty around the estimate for these two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AA5uoqAKUSx"
   },
   "source": [
    "## Proxies for sensitive features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PSup7dMJjJg"
   },
   "source": [
    "We next investigate which of the features are highly predictive of the sensitive feature *race*; such features are called *proxies*.\n",
    "\n",
    "While in this tutorial we examine fairness issues through the **impact** of the machine-learning model on different populations, there are other concepts of fairness that seek to analyze how the **model might be using information** contained in the sensitive features, and which of the information uses are justified (often using causal reasoning). More pragmatically, certain uses of sensitive features (or proxies of it) might be illegal in some contexts.\n",
    "\n",
    "Another reason to understand the proxies is because they might explain why we see differences in impact on different groups even when our model does not have access to the sensitive features directly.\n",
    "\n",
    "In this section we briefly examine the identification of such proxies (but we don't go into legal or causality considerations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PHULa3eQEcn"
   },
   "source": [
    "In the United States, *Medicare* and *Medicaid* are joint federal and state programs to help qualified individuals pay for healthcare expenses. *Medicare* is available to people over the age of 65 and younger individuals with severe illnesses. *Medicaid* is available to all individuals under the age of 65 whose adjusted gross income falls below the Federal Poverty Line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNYx0OaVElUC"
   },
   "source": [
    "First, let's explore the relationship between patients who paid with *Medicaid* and our demographic features. Because *Medicaid* is available to low-income individuals, and race is correlated with socioeconomic status in the United States, we expect there to be a relationship between `race` and paying with *Medicaid*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "dR3eNMeY0HTi",
    "outputId": "aaa8498f-1f82-4003-c73c-ab6eaa3c2476"
   },
   "outputs": [],
   "source": [
    "sns.pointplot(y=\"medicaid\", x=\"race\", data=df, join=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTcAD0oxbarW"
   },
   "source": [
    "From our analysis, we see that paying with *Medicaid* does appear to have some relationship with the patient's race. *Caucasian* patients are the least likely to pay with *Medicaid* compared with other groups. If paying with *Medicaid* is a proxy for socioeconomic status, then the patterns we find align with our understanding of wealth and race in the United States."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdrS4By1dVk-"
   },
   "source": [
    "<a name=\"exercise-dataset\"></a>\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-sOl0rqblsb"
   },
   "source": [
    "Now, let's explore the relationship between paying with `medicare` and other demographic features. In the below sections, feel free to perform any analysis you would like to better understand the relationship between `medicare` and `race` and `gender` in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "lp02oUbWZx54",
    "outputId": "40c1cc55-625f-47d5-a21a-8950db45ab23"
   },
   "outputs": [],
   "source": [
    "sns.pointplot(y=\"medicare\", x=\"race\", data=df, ci=95, join=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "ONJcedCWULXN",
    "outputId": "1f0e82df-b12a-459b-9b77-13285ae44008"
   },
   "outputs": [],
   "source": [
    "sns.pointplot(y=\"medicare\", x=\"age\", data=df, ci=95, join=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "jXtY7_xdULLe",
    "outputId": "67948de6-24eb-4d61-9116-1d465534ee53"
   },
   "outputs": [],
   "source": [
    "sns.pointplot(y=\"medicare\", x=\"gender\", data=df, ci=95, join=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVSerRqDG3we"
   },
   "source": [
    "<a name=\"datasheets\"></a>\n",
    "## Datasheets for datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACLMWSAwGaLc"
   },
   "source": [
    "The _datasheets_ practice was proposed by [Gebru et al. (2018)](https://arxiv.org/abs/1803.09010). A datasheet of a given dataset documents the motivation behind the dataset creation, the dataset composition, collection process, recommended uses and many other characteristics. In the words of Gebru et al., the goal is to\n",
    "> facilitate better communication between dataset creators\n",
    "> and dataset consumers, and encourage the machine learning\n",
    "> community to prioritize transparency and accountability.\n",
    "\n",
    "In this section, we show how to fill in some of the sections of the datasheet for the dataset that we are using. The information is obtained directly from [Strack et al. (2014)](https://www.hindawi.com/journals/bmri/2014/781670/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcc4cUMhZnCb"
   },
   "source": [
    "### Example sections of a datasheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TIPJIhX1IKJ"
   },
   "source": [
    "**For what purpose was the dataset created?** *Was there a specific task in mind? Was there a specific gap that needed to be filled?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBNKbKJQJbmf"
   },
   "source": [
    "In the words of the dataset authors:\n",
    "> [...] the management of hyperglycemia in the hospitalized patient has a significant bearing on outcome, in terms of both morbidity and mortality. This recognition has led to the development of formalized protocols in the intensive care unit (ICU) setting [...] However, the same cannot be said for most non-ICU inpatient admissions. [...] there are few national assessments of diabetes care in the hospitalized patient which could serve as a baseline for change [in the non-ICU protocols]. The present analysis of a large clinical database was undertaken to examine historical patterns of diabetes care in patients with diabetes admitted to a US hospital and to inform future directions which might lead to improvements in patient safety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iA-XRu0o1ErL"
   },
   "source": [
    "**Who created the dataset (e.g., which team) and on behalf of which entity?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQSHxl26LQkt"
   },
   "source": [
    "The dataset was created by [Strack et al. (2014)](https://www.hindawi.com/journals/bmri/2014/781670/): a team of researchers from a variety of disciplines, ranging from computer science to public health, from three institutions (Virginia Commonwealth University, University of Cordoba, and Polish Academy of Sciences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Qx32mSJG3zP"
   },
   "source": [
    "#### **Composition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RS2V8001F3E"
   },
   "source": [
    "**What do the instances that comprise the dataset represent?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPy_TXp_1Gub"
   },
   "source": [
    "Each instance in this dataset represents a hospital admission for diabetic patient (diabetes was entered as a possible diagnosis for the patient) whose hospital stay lasted between one to fourteen days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOb0FPeOJqxm"
   },
   "source": [
    "**Is any information missing from individual instances?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vlZWeQjJq8w"
   },
   "source": [
    "The features `Payer Code` and `Medical Specialty` have 40,255 and 49,947 missing values, respectively. For `Payer Code`, these missing values are reflected in the category *Unknown*. For `Medical Specialty`, these missing values are reflecting in the category *Missing*. \n",
    "\n",
    "For our demographic features, we are missing the `Gender` information for three patients in the dataset. These three records were dropped from our final dataset. Regarding `Race`, the 2,271 missing values were recoded into the `Unknown` race category. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh0lLV6mJrSp"
   },
   "source": [
    "**Does the dataset identify any subpopulations (e.g., by age, gender)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-ZnQfibJrcQ"
   },
   "source": [
    "Patients are identified by gender, age group, and race. \n",
    "\n",
    "For gender, patients are identified as Male, Female, or Unknown. There were only three instances where the patient gender is *Unknown*, so these records were removed from our dataset.\n",
    "\n",
    "Gender | Count| Percentage\n",
    "------ | ------|----------\n",
    "Male      | 47055     | 46.2%\n",
    "Female     |  54708     | 53.7%  \n",
    "\n",
    "\n",
    "\n",
    "For age group, patients are binned into three age buckets: *30 years or younger*, *30-60 years*, *Older than 60 years*.\n",
    "\n",
    "Age Group |Count| Percentage\n",
    "------ | ------|----------\n",
    "30 years or younger      | 2509     | 2.4%\n",
    "30-60 years       | 30716   | 30.2%\n",
    "Older than 60 years      |   68538    | 67.4%  \n",
    "\n",
    "\n",
    "For race, patients are identified as *AfricanAmerican*, *Caucasian*, and *Other*. For individuals whose race information was not collected during hospital admission, their race is listed as *Unknown*.\n",
    "\n",
    "Race | Count| Percentage\n",
    "------ | ------|----------\n",
    "Caucasian      | 76099     | 74.8%\n",
    "AfricanAmerican     |  19210     | 18.9%  \n",
    "Other        |     4183         |  4.1%\n",
    "Unknown        |    2271          | 2.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzXw0egqG4J4"
   },
   "source": [
    "#### **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGfxGcI21Fyj"
   },
   "source": [
    "**Was any preprocessing/cleaning/labeling of the data done?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jO4Pf911GrL"
   },
   "source": [
    "For the `race` feature, the categories of *Asian* and *Hispanic* and *Other* were merged into the *Other* category. The `age` feature was bucketed into 30-year intervals (*30 years and below*, *30 to 60 years*, and *Over 60 years*). The `discharge_disposition_id` was binarized into a boolean outcome on whether an patient was discharged to home.\n",
    "\n",
    "The full preprocessing code is provided in the file `preprocess.py` of the tutorial [GitHub repository](https://github.com/fairlearn/talks/blob/main/2021_scipy_tutorial/).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8b5nXfPIA7a"
   },
   "source": [
    "#### **Uses**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YH8evvN1HX2"
   },
   "source": [
    "**Has the dataset been used for any tasks already?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8RW1LKW1Hbg"
   },
   "source": [
    "This dataset has been used by [Strack et al. (2014)](https://www.hindawi.com/journals/bmri/2014/781670/) to model the relationship between patient readmission and HbA1c measurement during admission, based on primary medical diagnosis.\n",
    "\n",
    "The dataset is publicly available through the UCI Machine Learning Repository and, as of May 2021, has received over 350,000 views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB7zfhA1UKiW"
   },
   "source": [
    "# Training the initial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f7jZOzGX24Z"
   },
   "source": [
    "We next train a classification model to predict our target variable (_readmission within 30 days_) while optimizing _balanced accuracy_.\n",
    "\n",
    "What kind of model should we train? Deep neural nets? Random forests? Logistic regression?\n",
    "\n",
    "There are a variety of considerations. We highlight two:\n",
    "\n",
    "* **Interpretability.** Interpretability is tightly linked with questions of fairness. There are several reasons why it is important to have interpretable models that are open to the stakeholder scrutiny:\n",
    "  * It allows discovery of fairness issues that were not discovered by the data science team.\n",
    "  * It provides a path toward recourse for those that are affected by the model.\n",
    "  * It allows for a *face validity* check, a \"sniff test\", by experts to verify that the model \"makes sense\" (at the face value). While this step is subjective, it is really important when the model is applied to different populations than those on which the assessment was conducted.\n",
    "\n",
    "* **Model expressiveness.** How well can the model separate positive examples from negative examples? How well can it do so given the available dataset size? Can it do so across all groups or does it need to trade off performance on one group against performance on another group?\n",
    "\n",
    "Some additional considerations are training time (this impacts the ability to iterate), familiarity (this impacts the ability to fine tune and debug), and carbon footprint (this impacts the Earth climate both directly and indirectly by normalizing unnecessarily heavy workloads)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hbsqXap9Mzp"
   },
   "source": [
    "### Decision point: Model type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iftAwdfoVDM0"
   },
   "source": [
    "We will use a logistic regression model. Our reasoning:\n",
    "\n",
    "* **Interpretability**. Logistic models over a small number of variables (as used here) are highly interpretable in the sense that stakeholders can simulate them easily.\n",
    "\n",
    "* **Model expressiveness**. Logistic regression predictions are described by a linear weighting of the feature values. The concern might be that this is too simple. The previous work by [Strack et al. (2014)](https://www.hindawi.com/journals/bmri/2014/781670/), which also used a logistic model to predict readmission rates concluded that a slightly more expressive model might be useful (their analysis uncovered eight pairwise interactions that were significant, see their Table 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52wWLOFoXkho"
   },
   "source": [
    "## Prepare training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCwtDyx8yqSG"
   },
   "source": [
    "As we mentioned in the task definition, our target variable is **readmission within 30 days**, and our sensitive feature for the purposes of fairness assessment is **race**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpiSRj2JyqSH"
   },
   "outputs": [],
   "source": [
    "target_variable = \"readmit_30_days\"\n",
    "demographic = [\"race\", \"gender\"]\n",
    "sensitive = [\"race\"]\n",
    "# If multiple sensitive features are chosen, the rest of the script considers intersectional groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uaNqDyqvi1QE"
   },
   "outputs": [],
   "source": [
    "Y, A = df.loc[:, target_variable], df.loc[:, sensitive]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niu49A9YXQmf"
   },
   "source": [
    "We next drop the features that we don't want to use in our model and expand the categorical features into 0/1 indicators (\"dummies\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXyRuCsri1cY"
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df.drop(columns=[\n",
    "    \"race\",\n",
    "    \"race_all\",\n",
    "    \"discharge_disposition_id\",\n",
    "    \"readmitted\",\n",
    "    \"readmit_binary\",\n",
    "    \"readmit_30_days\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "id": "kAA0sIhQUWFa",
    "outputId": "73d62224-bf2e-40d2-a29a-8909e26d3cf9"
   },
   "outputs": [],
   "source": [
    "X.head() # sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATzi8cKCD7V3"
   },
   "source": [
    "We split our data into a training and test portion. The test portion will be used to evaluate our performance metric (i.e., balanced accuracy), but also for fairness assessment. The split is half/half for training and test to ensure that we have sufficient sample sizes for fairness assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wpnURazmJ4-"
   },
   "outputs": [],
   "source": [
    "random_seed = 445\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgl_b-CUl7TW"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, A_train, A_test, df_train, df_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    A,\n",
    "    df,\n",
    "    test_size=0.50,\n",
    "    stratify=Y,\n",
    "    random_state=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSMbXR9iVqr8"
   },
   "source": [
    "Our performance metric is **balanced accuracy**, so for the purposes of training (but not evaluation!) we will resample the data set, so that it has the same number of positive and negative examples. This means that we can use estimators that optimize standard accuracy (although some estimators allow the use us importance weights).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPNQpb2ZN1ku"
   },
   "source": [
    "Because we are downsampling the number of negative examples, we create a training dataset with a significantly lower number of data points. For more complex machine learning models, this lower number of training data points may affect the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1aVgzyFNa4B"
   },
   "outputs": [],
   "source": [
    "def resample_dataset(X_train, Y_train, A_train):\n",
    "\n",
    "  negative_ids = Y_train[Y_train == 0].index\n",
    "  positive_ids = Y_train[Y_train == 1].index\n",
    "  balanced_ids = positive_ids.union(np.random.choice(a=negative_ids, size=len(positive_ids)))\n",
    "\n",
    "  X_train = X_train.loc[balanced_ids, :]\n",
    "  Y_train = Y_train.loc[balanced_ids]\n",
    "  A_train = A_train.loc[balanced_ids, :]\n",
    "  return X_train, Y_train, A_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ogw-r3DQsds"
   },
   "outputs": [],
   "source": [
    "X_train_bal, Y_train_bal, A_train_bal = resample_dataset(X_train, Y_train, A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRddJS7XXv5n"
   },
   "source": [
    "## Save descriptive statistics of training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZ-T4lGxX0IQ"
   },
   "source": [
    "We next evaluate and save descriptive statistics of the training dataset. These will be provided as part of _model cards_ to document our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "n3GhcUCm2LjD",
    "outputId": "9a3be0c7-99ae-4de6-b61b-cf0fd0bd6b0e"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=\"race\", data=A_train_bal)\n",
    "plt.title(\"Sensitive Attributes for Training Dataset\")\n",
    "sensitive_train = figure_to_base64str(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "lIp3j8fD2LjE",
    "outputId": "48b4cbdf-c0ad-4a5b-d5f2-987d6b75229b"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=Y_train_bal)\n",
    "plt.title(\"Target Label Histogram for Training Dataset\")\n",
    "outcome_train = figure_to_base64str(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "czIGZYhk2LjF",
    "outputId": "07490aa7-c22e-4614-8580-be8b8b6db229"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=\"race\", data=A_test)\n",
    "plt.title(\"Sensitive Attributes for Testing Dataset\")\n",
    "sensitive_test = figure_to_base64str(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "YjhW0t9-2LjF",
    "outputId": "0785b4e1-88b8-4ab3-cb0b-9bf1d3df5ffe"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=Y_test)\n",
    "plt.title(\"Target Label Histogram for Test Dataset\")\n",
    "outcome_test = figure_to_base64str(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4V523GQbYobT"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7jwN2cVbO0g"
   },
   "source": [
    "We train a logistic regression model and save its predictions on test data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6nKDzt164vw"
   },
   "outputs": [],
   "source": [
    "unmitigated_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessing\", StandardScaler()),\n",
    "    (\"logistic_regression\", LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "ld9clGbHl7tv",
    "outputId": "89fa7f0d-680f-424f-a7cd-96686987a943"
   },
   "outputs": [],
   "source": [
    "unmitigated_pipeline.fit(X_train_bal, Y_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ok-eREU0xbAD"
   },
   "outputs": [],
   "source": [
    "Y_pred_proba = unmitigated_pipeline.predict_proba(X_test)[:,1]\n",
    "Y_pred = unmitigated_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkA0K8KV0HeD"
   },
   "source": [
    "Check model performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "nz7QJOLx0RVH",
    "outputId": "ef446998-8269-4e9b-c8ba-43a777b947d8"
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve of probabilistic predictions\n",
    "plot_roc_curve(unmitigated_pipeline, X_test, Y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxYppCAy1owq",
    "outputId": "b9febbe3-8016-43a7-c14a-98a94f05716a"
   },
   "outputs": [],
   "source": [
    "# Show balanced accuracy rate of the 0/1 predictions\n",
    "balanced_accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmWrDs5N2HVD"
   },
   "source": [
    "As we see, the performance of the model is well above the performance of a coin flip (whose performance would be 0.5 in both cases), albeit it is quite far from a perfect classifier (whose performance would be 1.0 in both cases).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmhwS1Z9VnK9"
   },
   "source": [
    "## Inspect the coefficients of trained model\n",
    "\n",
    "We check the coefficients of the fitted model to make sure that they \"makes sense\". While subjective, this step is important and helps catch mistakes and might point out to some fairness issues. However, we will systematically assess the fairness of the model in the next section.\n",
    "\n",
    "*Note that coefficients are also a proxy for \"feature importance\", but this interpretation can be misleading when features are highly correlated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "Owzkar8R9Cyy",
    "outputId": "7c073a93-e3e5-4f22-e089-217f97967df7"
   },
   "outputs": [],
   "source": [
    "coef_series = pd.Series(data=unmitigated_pipeline.named_steps[\"logistic_regression\"].coef_[0], index=X.columns)\n",
    "coef_series.sort_values().plot.barh(figsize=(4, 12), legend=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX8CrWjhD7MB"
   },
   "source": [
    "# **Fairness assessment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CS9-jaxtxh2"
   },
   "source": [
    "## Measuring fairness-related harms\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8TMm9w8duVY"
   },
   "source": [
    "The goal of fairness assessment is to answer the question: *Which groups of people may be disproportionately negatively impacted by an AI system and in what ways?*\n",
    "\n",
    "The steps of the assesment are as follows:\n",
    "1. Identify harms\n",
    "2. Identify the groups that might be harmed\n",
    "3. Quantify harms\n",
    "4. Compare quantified harms across the groups\n",
    "\n",
    "We next examine these four steps in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6hMFmzmPbL6"
   },
   "source": [
    "### 1. Identify harms\n",
    "\n",
    "For example, in a system for screening job applications, qualified candidates that are automatically rejected experience an allocation harm. In a speech-to-text transcription system, high error rates constitute harm in the quality of service.\n",
    "\n",
    "**In the health care scenario**, the patients that would benefit from a care management program, but are not recommended for it experience an allocation harm. In the context of the classification scenario these are **FALSE NEGATIVES**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqqUk1mjPnpM"
   },
   "source": [
    "### 2. Identify the groups that might be harmed\n",
    "\n",
    "In most applications, we consider demographic groups including historically marginalized groups (e.g., based on gender, race, ethnicity). We should also consider groups that are relevant to a particular application. For example, for speech-to-text transcription, groups based on the regional dialect or being a native or a non-native speaker.\n",
    "\n",
    "It is also important to consider group intersections, for example, in addition to considering groups according to gender and groups according to race, it is also important to consider their intersections (e.g., Black women, Latinx nonbinary people, etc.).\n",
    "\n",
    "**In the health care scenario**, based on the previous work, we focus on groups defined by **RACE**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmvSqI3dPrVk"
   },
   "source": [
    "### 3. Quantify harms\n",
    "\n",
    "Define metrics that quantify harms or benefits:\n",
    "\n",
    "* In job screening scenario, we need to quantify the number of candidates that are classified as \"negative\" (not recommended for the job), but whose true label is \"positive\" (they are qualified). One possible metric is the **false negative rate**: fraction of qualified candidates that are screened out.\n",
    "\n",
    "* In speech-to-text scenario, the harm could be measured by **word error rate**, number of mistakes in a transcript divided by the overall number of words.\n",
    "\n",
    "* **In the health care scenario**, we could consider two metrics for quantifying harms / benefits:\n",
    "  * **false negative rate**: fraction of patients that are readmitted within 30 days, but that are not recommended for the care management program; this quantifies harm\n",
    "  * **selection rate**: overall fraction of patients that are recommended for the care management program (regardless of whether they are readmittted with 30 days or no); this quantifies benefit; here the assumption is that all patients benefit similarly from the extra care.\n",
    "\n",
    "There are several reasons for including selection rate in addition to false negative rate. We would like to monitor how the benefits are allocated, focusing on groups that might be disadvantaged. Another reason is to get extra robustness in our assessement, because our measure (i.e., readmission within 30 days) is only an imperfect measure of our construct (who is most likely to benefit from the care management program). The auxiliary metrics, like selection rate, may alert us to large disparities in how the benefit is allocated, and allow us to catch issues that we might have missed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpJXt6miPvRX"
   },
   "source": [
    "### 4. Compare quantified harms across the groups\n",
    "\n",
    "The workhorse of fairness assessment are _disaggregated metrics_, which are **metrics evaluated on slices of data**. For example, to measure harms due to errors, we would begin by evaluating the errors on each slice of the data that corresponds to a group we identified in Step 2.\n",
    "If some of the groups are seeing much larger errors than other groups, we would flag this as a fairness harm.\n",
    "\n",
    "To summarize the disparities in errors (or other metrics), we may want to report quantities such as the **difference** or **ratio** of the metric values between the best and the worst slice. In settings where the goal is to guarantee certain minimum quality of service (such as speech recognition), it is also meaningful to report the **worst performance** across all considered groups.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Is_zdXvnW0s"
   },
   "source": [
    "For example, when comparing false negative rate across groups defined by race, we may summarize our findings with a table like the following:\n",
    "\n",
    "| | false negative rate<br>(FNR) |\n",
    "|---|---|\n",
    "| AfricanAmerican | 0.43 |\n",
    "| Caucasian | 0.44 |\n",
    "| Other | 0.52 |\n",
    "| Unknown | 0.67 |\n",
    "| | |\n",
    "|_largest difference_| 0.24 &nbsp;&nbsp;(best is 0.0)|\n",
    "|_smallest ratio_| 0.64 &nbsp;&nbsp;(best is 1.0)|\n",
    "|_maximum_<br>_(worst-case) FNR_|0.67|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CjHlopBDgSG"
   },
   "source": [
    "## Fairness assessment with `MetricFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epJO2baHV2Dy"
   },
   "source": [
    "Fairlearn provides the data structure called `MetricFrame` to enable evaluation of disaggregated metrics. We will show how to use a `MetricFrame` object to assess the trained `LogisticRegression` classifier for potential fairness-related harms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iiAYRvoduPh",
    "outputId": "3287b55e-3610-424d-a99d-a6e19c2b1693"
   },
   "outputs": [],
   "source": [
    "# In its simplest form MetricFrame takes four arguments:\n",
    "#    metric_function with signature metric_function(y_true, y_pred)\n",
    "#    y_true: array of labels\n",
    "#    y_pred: array of predictions\n",
    "#    sensitive_features: array of sensitive feature values\n",
    "\n",
    "mf1 = MetricFrame(metrics=false_negative_rate,\n",
    "                  y_true=Y_test,\n",
    "                  y_pred=Y_pred,\n",
    "                  sensitive_features=df_test['race'])\n",
    "\n",
    "# The disaggregated metrics are stored in a pandas Series mf1.by_group:\n",
    "\n",
    "mf1.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tl1Qxt1fT2v",
    "outputId": "e2af1585-7602-4664-f396-2f0d395d4ad2"
   },
   "outputs": [],
   "source": [
    "# The largest difference, smallest ratio and worst-case performance are accessed as\n",
    "#   mf1.difference(), mf1.ratio(), mf1.group_max()\n",
    "\n",
    "print(f\"difference: {mf1.difference():.3}\\n\"\n",
    "      f\"ratio: {mf1.ratio():.3}\\n\"\n",
    "      f\"max across groups: {mf1.group_max():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "R2zmBHo5gk-F",
    "outputId": "e970f25b-7218-4430-8969-d8f891ddce27"
   },
   "outputs": [],
   "source": [
    "# You can also evaluate multiple metrics by providing a dictionary\n",
    "\n",
    "metrics_dict = {\n",
    "    \"selection_rate\": selection_rate,\n",
    "    \"false_negative_rate\": false_negative_rate,\n",
    "    \"balanced_accuracy\": balanced_accuracy_score,\n",
    "}\n",
    "\n",
    "metricframe_unmitigated = MetricFrame(metrics=metrics_dict,\n",
    "                  y_true=Y_test,\n",
    "                  y_pred=Y_pred,\n",
    "                  sensitive_features=df_test['race'])\n",
    "\n",
    "# The disaggregated metrics are then stored in a pandas DataFrame:\n",
    "\n",
    "metricframe_unmitigated.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hc29jRJrhlSC",
    "outputId": "e2ffee50-2764-434f-d322-2a8fdafe6a09"
   },
   "outputs": [],
   "source": [
    "# The largest difference, smallest ratio, and the maximum and minimum values\n",
    "# across the groups are then all pandas Series, for example:\n",
    "\n",
    "metricframe_unmitigated.difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "id": "bVbjFa4Aig9Y",
    "outputId": "228f2de1-4de8-4059-b166-c75550e52de2"
   },
   "outputs": [],
   "source": [
    "# You'll probably want to view them transposed:\n",
    "\n",
    "pd.DataFrame({'difference': metricframe_unmitigated.difference(),\n",
    "              'ratio': metricframe_unmitigated.ratio(),\n",
    "              'group_min': metricframe_unmitigated.group_min(),\n",
    "              'group_max': metricframe_unmitigated.group_max()}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "DvjRBIcjjSkl",
    "outputId": "0aebd5a1-f287-4d75-cdd3-8af1daf1d190"
   },
   "outputs": [],
   "source": [
    "# You can also easily plot all of the metrics using DataFrame plotting capabilities\n",
    "\n",
    "metricframe_unmitigated.by_group.plot.bar(subplots=True, layout= [1,3], figsize=(12, 4),\n",
    "                      legend=False, rot=-45, position=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5C3SITjuPUm"
   },
   "source": [
    "According to the above bar chart, it seems that the group *Unknown* is selected for the care management program less often than other groups as reflected by the selection rate. Also this group experiences the largest false negative rate, so a larger fraction of group members that are likely to benefit from the care management program are not selected. Finally, the balanced accuracy on this group is also the lowest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2Qs68rv2_Vg"
   },
   "source": [
    "We observe disparity, even though we did not include race in our model. There's a variety of reasons why such disparities may occur. It could be due to representational issues (i.e., not enough instances per group), or because the feature distribution itself differs across groups (i.e., different relationship between features and target variable, obvious example would be people with darker skin in facial recognition systems, but can be much more subtle). Real-world applications often exhibit both kinds of issues at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_1Rm8PbPmmk"
   },
   "source": [
    "<a name=\"train-other-models\"></a>\n",
    "## Exercise: Train other fairness-unaware models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeQF5qT6Qs-C"
   },
   "source": [
    "In this section, you'll be training your own fairness-unaware model and evaluate the model using the `MetricFrame` for fairness-related harms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsHy-Os0oVQU"
   },
   "source": [
    "We encourage you to explore the model's performance across different sensitive features (such as `age` or `gender`) as well as different model performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61GU_zrFSC-6"
   },
   "source": [
    "1.) First, let's train our machine learning model. We'll create a `HistGradientBoostingClassifier` and fit it to the balanced training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSIDkuV4_Rou"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Create your model here\n",
    "clf = HistGradientBoostingClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(__________, ________)\n",
    "exercise_pred = clf.predict(______)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fnnles-p6OXr"
   },
   "source": [
    "2.) Next, let's evaluate the fairness of the model using the `MetricFrame`. In the below cells, create a `MetricFrame` that looks at the following metrics:\n",
    "\n",
    "\n",
    "*   _Count_: The number of data points belonging to each sensitive feature category.\n",
    "*   _False Positive Rate_: $\\dfrac{FP}{FP+TN}$\n",
    "*  _Recall Score_: $\\dfrac{TP}{TP+FN}$\n",
    "\n",
    "As an extra challenge, you can use the prediction probabilities to compute the _ROC AUC Score_ for each sensitive group pair.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcf-x1oA_jP5"
   },
   "outputs": [],
   "source": [
    "#Define additional fairness metrics of interest here\n",
    "exercise_metrics = {\n",
    "    \"count\": count,\n",
    "    \"false_positive_rate\": _______,\n",
    "    \"recall_score\": _______\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bll-8GAWJF6p"
   },
   "source": [
    "Now, let's create our `MetricFrame` using the metrics listed above with the sensitive groups of `race` and `gender`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAjzjCqh_fNx"
   },
   "outputs": [],
   "source": [
    "metricframe_exercise = MetricFrame(\n",
    "    metrics=__________,\n",
    "    y_true=Y_test,\n",
    "    y_pred=__________,\n",
    "    sensitive_features=_____\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeghVCbLZOf5"
   },
   "source": [
    "3.) Finally, play around with the plotting capabilities of the `MetricFrame` in the below section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nd4D17ME_hB2"
   },
   "outputs": [],
   "source": [
    "metricframe_exercise._______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xaLx6Br_hyc"
   },
   "outputs": [],
   "source": [
    "# Plot some of the performance disparities here\n",
    "metricframe_exercise.by_group.____.bar(subplots=_____, layout=[1,4], figsize=(12, 4), legend=False, rot=-45, position=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me1ocEi2kEgw"
   },
   "source": [
    "The charts above are based on test data, so without any uncertainty quantification (such as error bars or confidence intervals), we cannot reliably compare these data statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZqVGZkam1eH"
   },
   "source": [
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dgITdRiD7Yu"
   },
   "source": [
    "# **Mitigating fairness-related harms in ML models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbUSG1jVA06G"
   },
   "source": [
    "We have found that the logistic regression predictor leads to a large difference in false negative rates between the groups. We next look at **algorithmic mitigation strategies** of this fairness issue (and similar ones).\n",
    "\n",
    "*Note that while we currently focus on the training stage of the AI lifecycle mitigation should not be limited to this stage. In fact, we have already discussed mitigation strategies that are applicable at the task definition stage (e.g., checking for construct validity) and data collection stage (e.g., collecting more data).*\n",
    "\n",
    "Within the model training stage, mitigation may occur at different steps relative to model training:\n",
    "\n",
    "* **Preprocessing**: A mitigation algorithm is applied to transform the input data to the training algorithm; for example, some strategies seek to remove and dependence between the input features and sensitive features.\n",
    "\n",
    "* **At training time**: The model is trained by an (optimization) algorithm that seeks to satisfy fairness constraints.\n",
    "\n",
    "* **Postprocessing**: The output of a trained model is transformed to mitigate fairness issues; for example, the predicted probability of readmission is thresholded according to a group-specific threshold.\n",
    "\n",
    "We will now dive into two algorithms: a postprocessing approach and a reductions approach (which is a training-time algorithm). Both of them are in fact **meta-algorithms** in the sense that they act as wrappers around *any* standard (fairness-unaware) machine learning algorithms. This makes them quite versatile in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rX8QycCL0mJj"
   },
   "source": [
    "## Postprocessing with `ThresholdOptimizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRZfSFzcFaXP"
   },
   "source": [
    "**Postprocessing** techniques are a class of unfairness-mitigation algorithms that take an already trained model and a dataset as an input and seek to fit a transformation function to model's outputs to satisfy some (group) fairness constraint(s). They might be the only feasible unfairness mitigation approach when developers cannot influence training of the model, due to practical reasons or due to security or privacy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PgzZkK9Wbni"
   },
   "source": [
    "Here we use the `ThresholdOptimizer` algorithm from Fairlearn, which follows the approach of [Hardt, Price, and Srebro (2016)](https://arxiv.org/abs/1610.02413).\n",
    "\n",
    "`ThresholdOptimizer` takes in an existing (possibly pre-fit) machine learning model whose predictions act as a scoring function and identifies a separate thrceshold for each group in order to optimize some specified objective metric (such as **balanced accuracy**) subject to specified fairness constraints (such as **false negative rate parity**). Thus, the resulting classifier is just a suitably thresholded version of the underlying machinelearning model.\n",
    "\n",
    "The constraint **false negative rate parity** requires that all the groups have equal values of false negative rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFOovaN7AwDr"
   },
   "source": [
    "To instatiate our `ThresholdOptimizer`, we pass in:\n",
    "\n",
    "*   An existing `estimator` that we wish to threshold. \n",
    "*   The fairness `constraints` we want to satisfy.\n",
    "*   The `objective` metric we want to maximize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8je0grKPWHhy"
   },
   "outputs": [],
   "source": [
    "# Now we instantite ThresholdOptimizer with the logistic regression estimator\n",
    "postprocess_est = ThresholdOptimizer(\n",
    "    estimator=unmitigated_pipeline,\n",
    "    constraints=\"false_negative_rate_parity\",\n",
    "    objective=\"balanced_accuracy_score\",\n",
    "    prefit=True,\n",
    "    predict_method='predict_proba'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDD86L7eCSe0"
   },
   "source": [
    "In order to use the `ThresholdOptimizer`, we need access to the sensitive features **both during training time and once it's deployed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "id": "VCHJBB7x1rAK",
    "outputId": "dfc6338e-2be0-4007-bde2-fd8544cc4a89"
   },
   "outputs": [],
   "source": [
    "postprocess_est.fit(X_train_bal, Y_train_bal, sensitive_features=A_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YscNZsYU1rCY"
   },
   "outputs": [],
   "source": [
    "# Record and evaluate the output of the trained ThresholdOptimizer on test data\n",
    "\n",
    "Y_pred_postprocess = postprocess_est.predict(X_test, sensitive_features=A_test)\n",
    "metricframe_postprocess = MetricFrame(\n",
    "    metrics=metrics_dict,\n",
    "    y_true=Y_test,\n",
    "    y_pred=Y_pred_postprocess,\n",
    "    sensitive_features=A_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_izbGv6tQ1KD"
   },
   "source": [
    "We can now inspect how the metric values differ between the postprocessed model and the unmitigated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "-9mtWyWc1rH5",
    "outputId": "982dcb16-b82f-42bc-addb-1caaf2d0aa09"
   },
   "outputs": [],
   "source": [
    "pd.concat([metricframe_unmitigated.by_group,\n",
    "           metricframe_postprocess.by_group],\n",
    "           keys=['Unmitigated', 'ThresholdOptimizer'],\n",
    "           axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzPCUFsXPU_S"
   },
   "source": [
    "We next zoom in on differences between the largest and the smallest metric values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "dsC4v8Ap1rKt",
    "outputId": "ebf489b6-07ac-45d5-ae2b-848d6488dbb4"
   },
   "outputs": [],
   "source": [
    "pd.concat([metricframe_unmitigated.difference(),\n",
    "           metricframe_postprocess.difference()],\n",
    "          keys=['Unmitigated: difference', 'ThresholdOptimizer: difference'],\n",
    "          axis=1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hhi_RSxSRoyg"
   },
   "source": [
    "As we see, `ThresholdOptimizer` was able to substantiallydecrease the difference between the values of false negative rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GarQvopkVN2S"
   },
   "source": [
    "Finally, we save the disagregated statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "EsTehBH2SW7f",
    "outputId": "3cf05d43-1389-41a2-e2dc-7e7511833c2b"
   },
   "outputs": [],
   "source": [
    "metricframe_postprocess.by_group.plot.bar(subplots=True, layout=[1,3], figsize=(12, 4), legend=False, rot=-45, position=1.5)\n",
    "postprocess_performance = figure_to_base64str(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umd3slsDmk0d"
   },
   "source": [
    "As an optional validation, we could check to see if the `ThresholdOptimizer` more closely satisfies the `false_negative_rate_difference` constraint on the training data than on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2vLNUnK_P66"
   },
   "source": [
    "<a name=\"exercise-threshold\"></a>\n",
    "### Exercise: ThresholdOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YUembo02yQ8"
   },
   "source": [
    "In this exercise, we will create a `ThresholdOptimizer` by constraining the *true positive rate* (also known as the *recall score*). For any model, the *true positive rate* + *false negative rate* = 1. \n",
    "\n",
    "By trying to achieve the *true positive rate parity*, we should produce a `ThresholdOptimizer` with the same performance as our original `ThresholdOptimizer`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJxE2eMuNF3_"
   },
   "source": [
    "#### 1.) Create a new ThresholdOptimizer with the constraint `true_positive_rate_parity` <br> and objective function `balanced_accuracy_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zD5kQu6gqyl6"
   },
   "outputs": [],
   "source": [
    "# Instatitate ThresholdOptimizer\n",
    "thresopt_exercise = ThresholdOptimizer(\n",
    "    estimator=______________,\n",
    "    constraints=____________,\n",
    "    objective=\"balanced_accuracy_score\",\n",
    "    prefit=True,\n",
    "    predict_method='predict_proba'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxdAikCKqyuW"
   },
   "outputs": [],
   "source": [
    "# Fit to data and predict on test data\n",
    "thresopt_exercise.____(X_train_bal, Y_train_bal, sensitive_features=_______)\n",
    "threshopt_pred = thresopt_exercise._________(X_test, sensitive_features=_______)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBLD77OENFN-"
   },
   "source": [
    "#### 2.) Create a new `MetricFrame` object to process the results of this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B12UNsZErImo"
   },
   "outputs": [],
   "source": [
    "thresop_metricframe = MetricFrame(\n",
    "    metrics=metrics_dict,\n",
    "    y_true=Y_test,\n",
    "    y_pred=____________,\n",
    "    sensitive_features=_______\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEyEie-9NEew"
   },
   "source": [
    "#### 3.) Compare the performance of the two `ThresholdOptimizers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uz9mS66YrWka"
   },
   "outputs": [],
   "source": [
    "# Visualize the performance of the new ThresholdOptimizer\n",
    "thresop_metricframe._______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oByEABVGrXWo"
   },
   "outputs": [],
   "source": [
    "# Compare the performance to the original ThresholdOptimizer\n",
    "metricframe_postprocess.______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAUI3LdQbBCs"
   },
   "source": [
    "Similar to many unfairness mitigation approaches, `ThresholdOptimizer` produces randomized classifiers. In certain situation, you may not be able to deploy such a randomized classifier in production. In these cases, the model may need to approximated by a deterministic equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NU_rncBQ0lab"
   },
   "source": [
    "## Reductions approach with `ExponentiatedGradient`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpyBnqNLZ-w_"
   },
   "source": [
    "With the `ThresholdOptimizer`, we took a fairness-unaware model and transformed the model's decision boundary to satisfy our fairness constraints. One limitation of `ThresholdOptimizer` is that it needs access to the sensitive features at deployment time.\n",
    "\n",
    "In this section, we will show how to use the _reductions_ approach of [Agarwal et. al (2018)](https://arxiv.org/abs/1803.02453) to obtain a model that satisfies the fairness constraints, but does not need access to sensitive features at deployment time.\n",
    "\n",
    "Terminology \"reductions\" refers to another kind of a wrapper approach, which instead of wrapping an already trained model, wraps any standard classification or regression algorithm, such as \n",
    "`LogisticRegression`. In other words, an input to a reduction algorithm is an object that supports training on any provided (weighted) dataset. In addition, a reduction algorithm receives a data set that includes sensitive features. The goal, like with post-processing, is to optimize a performance metric (such as classification accuracy) subject to fairness constraints (such as an upper bound on differences between false negative rates).\n",
    "\n",
    "The main reduction algorithm algorithm in Fairlearn is `ExponentiatedGradient`. It creates a sequence of reweighted datasets and retrains the wrapped model on each of them. The \n",
    "retraining process is guaranteed to find a model that satisfies the fairness constraints while optimizing the performance metric.\n",
    "\n",
    "The model returned by `ExponentiatedGradient` consists of several inner models, returned by the wrapped estimator. At deployment time, `ExponentiatedGradient` randomizes among these models according to a specific probability weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvT_qeduHCn8"
   },
   "source": [
    "To instantiate an `ExponentiatedGradient` model, we pass in two parameters:\n",
    "\n",
    "*   A base `estimator` (an object that supports training)\n",
    "*   Fairness `constraints` (an object of type `Moment`).\n",
    "\n",
    "The constraints supported by `ExponentiatedGradient` are more general than those supported by `ThresholdOptimizer`. For example, rather than requiring that false negative rates be equal, it is possible to specify the maxium allowed difference or ratio between the largest and the smallest value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ToZdrYen0tJZ"
   },
   "outputs": [],
   "source": [
    "expgrad_est = ExponentiatedGradient(\n",
    "    estimator=LogisticRegression(max_iter=1000, random_state=random_seed),\n",
    "    constraints=TruePositiveRateParity(difference_bound=0.02)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLFk3SCxrskU"
   },
   "source": [
    "The constraints above are expressed for the true positive parity, they require that the difference between the largest and the smallest true positive rate (TPR) across all groups be at most 0.02. Since false negative rate (FNR) is equal to 1-TPR, this is equivalent to requiring that the difference between the largest and smallest FNR be at most 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "UFSF5Wn-3M-H",
    "outputId": "43f639cf-58ef-4425-b772-3daec9d9ff3d"
   },
   "outputs": [],
   "source": [
    "# Fit the exponentiated gradient model\n",
    "expgrad_est.fit(X_train_bal, Y_train_bal, sensitive_features=A_train_bal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsCeOlKFDQZZ"
   },
   "source": [
    "Similarly to `ThresholdOptimizer` the predictions of `ExponentiatedGradient` models are randomized. If we want to assure reproducible results, we can pass  `random_state` to the `predict` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "YYz7GAqf4cbp",
    "outputId": "72957bdb-41c5-41c6-b9c5-7f0ba8fdf610"
   },
   "outputs": [],
   "source": [
    "# Record and evaluate predictions on test data\n",
    "\n",
    "Y_pred_reductions = expgrad_est.predict(X_test, random_state=random_seed)\n",
    "metricframe_reductions = MetricFrame(\n",
    "    metrics=metrics_dict,\n",
    "    y_true=Y_test,\n",
    "    y_pred=Y_pred_reductions,\n",
    "    sensitive_features=A_test\n",
    ")\n",
    "metricframe_reductions.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "idYvm9lq4mh3",
    "outputId": "0848b976-8651-45ae-ddb3-b906b9417ac5"
   },
   "outputs": [],
   "source": [
    "# Evaluate the difference between the largest and smallest value of each metric\n",
    "metricframe_reductions.difference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpxYOozouVx9"
   },
   "source": [
    "While there is a decrease in the false negative rate difference from the unmitigated model, this decrease is not as substantial as with `ThresholdOptimizer`. Note, however, that `ThresholdOptimizer` was able to use the sensitive feature (i.e., race) at deployment time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzTeHhWG4nJJ"
   },
   "source": [
    "### Explore individual predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7qCmHeYKWGp"
   },
   "source": [
    "During the training process, the `ExponentiatedGradient` algorithm iteratively trains multiple inner models on a reweighted training dataset. The algorithm stores each of these predictors and then randomizes among them at deployment time.\n",
    "\n",
    "In many applications, the randomization is undesirable, and also using multiple inner models can pose issues for interpretability. However, the inner models that `ExponentiatedGradient` relies on span a variety of fairness-accuracy trade-offs, and they could be considered for stand-alone deployment: addressing the randomization and interpretability issues, while possibly offering additional flexibility thanks to a variety of trade-offs. \n",
    "\n",
    "In this section explore the performance of the individual predictors learned by the `ExponentiatedGradient` algorithm. First, note that since the base estimator was `LogisticRegression` all these predictors are different logistic regression models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "II_YtUZg3Ue4",
    "outputId": "76948908-4240-4d6b-d1db-38cbeda7d5be"
   },
   "outputs": [],
   "source": [
    "predictors = expgrad_est.predictors_\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3Bh2IVm4Ynj"
   },
   "outputs": [],
   "source": [
    "# Collect predictions by all predictors and calculate balanced error\n",
    "# as well as the false negative difference for all of them\n",
    "\n",
    "sweep_preds = [clf.predict(X_test) for clf in predictors]\n",
    "balanced_error_sweep = [1-balanced_accuracy_score(Y_test, Y_sweep) for Y_sweep in sweep_preds]\n",
    "fnr_diff_sweep = [false_negative_rate_difference(Y_test, Y_sweep, sensitive_features=A_test) for Y_sweep in sweep_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "r9TKGsNY8Myu",
    "outputId": "06146e22-ea39-4d70-9488-0dd9b03eddca"
   },
   "outputs": [],
   "source": [
    "# Show the balanced error / fnr difference values of all predictors on a raster plot  \n",
    "\n",
    "plt.scatter(balanced_error_sweep, fnr_diff_sweep, label=\"ExponentiatedGradient - Iterations\")\n",
    "for i in range(len(predictors)):\n",
    "  plt.annotate(str(i), xy=(balanced_error_sweep[i]+0.001, fnr_diff_sweep[i]+0.001))\n",
    "\n",
    "# Also include in the plot the combined ExponentiatedGradient model\n",
    "# as well as the three previously fitted models\n",
    "\n",
    "plt.scatter(1-balanced_accuracy_score(Y_test, Y_pred_reductions),\n",
    "            false_negative_rate_difference(Y_test, Y_pred_reductions, sensitive_features=A_test),\n",
    "            label=\"ExponentiatedGradient - Combined model\")\n",
    "plt.scatter(1-balanced_accuracy_score(Y_test, Y_pred),\n",
    "            false_negative_rate_difference(Y_test, Y_pred, sensitive_features=A_test),\n",
    "            label=\"Unmitigated\")\n",
    "plt.scatter(1-balanced_accuracy_score(Y_test, Y_pred_postprocess),\n",
    "            false_negative_rate_difference(Y_test, Y_pred_postprocess, sensitive_features=A_test),\n",
    "            label=\"ThresholdOptimizer\")\n",
    "\n",
    "plt.xlabel(\"Balanced Error Rate\")\n",
    "plt.ylabel(\"False Negative Rate Difference\")\n",
    "plt.legend(bbox_to_anchor=(1.9,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MozjJKkZqz_"
   },
   "source": [
    "<a name=\"exercise-reductions\"></a>\n",
    "### Exercise: Train an `ExponentiatedGradient` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLeGnB4juJsM"
   },
   "source": [
    "In this section, we will explore how changing the base model for the `ExponentiatedGradient` affects the overall performance of the classifier. \n",
    "\n",
    "We will instatiate a new `ExponentiatedGradient` classifier with a base `HistGradientBoostingClassifer` estimator. We will use the same `difference_bound` as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghjfKhtB3Kl9"
   },
   "source": [
    "1.) First, let's create our new `ExponentiatedGradient` instance in the cells below and fit it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNjDh4JUAc6i"
   },
   "outputs": [],
   "source": [
    "# Create ExponentiatedGradient instance here\n",
    "expgrad_exercise = ExponentiatedGradient(\n",
    "    estimator=_______,\n",
    "    constraints=TruePositiveRateParity(difference_bound=____)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFIAEkEBAc9P"
   },
   "outputs": [],
   "source": [
    "# Fit the new instance to the balanced training dataset\n",
    "expgrad_exercise.fit(________, _________, sensitive_features=________)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8H_UDlAs3M-D"
   },
   "source": [
    "2.) Now, let's compute the performance of the `ExponentiatedGradient` model and compare it with the performance of `ExponentiatedGradient` model with logistic regression as base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Qkfth4ZZBQV"
   },
   "outputs": [],
   "source": [
    "# Save the predictions and report the disagregated metrics\n",
    "# of the exponantiated gradient model\n",
    "Y_expgrad_exercise = expgrad_exercise.predict(X_test)\n",
    "mf_expgrad_exercise = MetricFrame(\n",
    "    metrics=________,\n",
    "    y_true=Y_test,\n",
    "    y_pred=_______,\n",
    "    sensitive_features=________\n",
    ")\n",
    "mf_expgrad_exercise.______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "Q1nPfHpWRwWZ",
    "outputId": "ea0e57d9-ed9e-4d03-fd5b-d57329fe1d1a"
   },
   "outputs": [],
   "source": [
    "# Compare with the disaggregated metric values of the\n",
    "# exponentiated gradient model based on logistic regression\n",
    "metricframe_reductions.by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Boo771yFUyJ7"
   },
   "source": [
    "3.) Next, calculate the balanced error rate and false negative rate difference of each of the inner models learned by this new `ExponentiatedGradient` classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jws7w2z6RWUM"
   },
   "outputs": [],
   "source": [
    "# Save the inner predictors of the new model\n",
    "predictors_exercise = expgrad_exercise.predictors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVTqrrSRAtGc"
   },
   "outputs": [],
   "source": [
    "# Compute the balanced error rate and false negative rate difference for each of the predictors on the test data.\n",
    "balanced_error_exercise = [(1 - ______(Y_test, pred.predict(X_test))) for pred in predictors_exercise]\n",
    "false_neg_exercise = [(______(Y_test, pred.predict(X_test), sensitive_features=_____)) for pred in predictors_exercise]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aos3gdHjDQEi"
   },
   "source": [
    "4.) Finally, let's plot the performances of these individual inner models. In the below cells, plot the individual inner predictors against the performance of their corresponding exponentiated gradient model as well as the unmitigated logistic regression model, and the `ThresholdOptimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "IxTjb8CeZqPI",
    "outputId": "8e06051c-4629-42f0-93f6-51931da2615b"
   },
   "outputs": [],
   "source": [
    "# Plot the individual predictors against the Unmitigated Model and the ThresholdOptimizer\n",
    "plt.scatter(balanced_error_exercise, false_neg_exercise,\n",
    "            label=\"ExponentiatedGradient - Iterations - Exercise\")\n",
    "for i in range(len(predictors_exercise)):\n",
    "  plt.annotate(str(i), xy=(balanced_error_exercise[i]+0.001, false_neg_exercise[i]+0.001))\n",
    "\n",
    "plt.scatter(1-balanced_accuracy_score(Y_test, Y_expgrad_exercise),\n",
    "            false_negative_rate_difference(Y_test, Y_expgrad_exercise, sensitive_features=A_test),\n",
    "            label=\"ExponentiatedGradient - Combined - Exercise\")\n",
    "plt.scatter(1-balanced_accuracy_score(Y_test, Y_pred),\n",
    "            false_negative_rate_difference(Y_test, Y_pred, sensitive_features=A_test),\n",
    "            label=\"Unmitigated\")\n",
    "plt.scatter(1-balanced_accuracy_score(Y_test, Y_pred_postprocess),\n",
    "            false_negative_rate_difference(Y_test, Y_pred_postprocess, sensitive_features=A_test),\n",
    "            label=\"ThresholdOptimizer\")\n",
    "\n",
    "plt.xlabel(\"Balanced Error Rate\")\n",
    "plt.ylabel(\"False Negative Rate Difference\")\n",
    "plt.legend(bbox_to_anchor=(1.9,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyknfgrsW1gi"
   },
   "source": [
    "## Comparing performance of different techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vee-7c2Tw33O"
   },
   "source": [
    "Now we have covered two different class of techniques for mitigating the fairness-related harms we found in our fairness-unaware model. In this section, we will compare the performance of the models we trained above across our key metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEXnEeLl7mgc"
   },
   "source": [
    "#### Model performance - by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNyCZxHJuXZV"
   },
   "outputs": [],
   "source": [
    "def plot_technique_comparison(mf_dict, metric):\n",
    "  \"\"\"\n",
    "  Plots a specified metric for a given dictionary of MetricFrames.\n",
    "  \"\"\"\n",
    "  mf_dict = {k:v.by_group[metric] for (k,v) in mf_dict.items()}\n",
    "  comparison_df = pd.DataFrame.from_dict(mf_dict)\n",
    "  comparison_df.plot.bar(figsize=(12, 6), legend=False)\n",
    "  plt.title(metric)\n",
    "  plt.xticks(rotation=0, ha='center');\n",
    "  plt.legend(bbox_to_anchor=(1.01,1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dNb-kI3uHzM"
   },
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    \"Reductions\": metricframe_reductions,\n",
    "    \"Unmitigated\": metricframe_unmitigated,\n",
    "    \"Postprocessing\": metricframe_postprocess\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "SweXBa-vEWFM",
    "outputId": "a230ed5d-2665-477d-adba-287c76020032"
   },
   "outputs": [],
   "source": [
    "plot_technique_comparison(test_dict, \"false_negative_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "FJOwW9db3wKe",
    "outputId": "a34b7c03-6bfc-4843-c67e-bc5997ebf86c"
   },
   "outputs": [],
   "source": [
    "plot_technique_comparison(test_dict, \"balanced_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "dACpOpm67K-m",
    "outputId": "65a6de47-9ca4-49d8-e97a-16ef1db40c02"
   },
   "outputs": [],
   "source": [
    "plot_technique_comparison(test_dict, \"selection_rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYkgAggpW11N"
   },
   "source": [
    "\n",
    "\n",
    "#### Model performance - overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3Rwe98m6Pv2"
   },
   "outputs": [],
   "source": [
    "overall_df = pd.DataFrame.from_dict({\n",
    "    \"Unmitigated\": metricframe_unmitigated.overall,\n",
    "    \"Postprocessing\": metricframe_postprocess.overall,\n",
    "    \"Reductions\": metricframe_reductions.overall\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "id": "44Q-lYTa6PzX",
    "outputId": "376ce1db-ead9-4984-83c6-7e59e815a1b2"
   },
   "outputs": [],
   "source": [
    "overall_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "yUaAejwa6P3F",
    "outputId": "c6d84d35-fdde-47be-af88-69881afaa163"
   },
   "outputs": [],
   "source": [
    "overall_df.transpose().plot.bar(subplots=True, layout= [1,3], figsize=(12, 5), legend=False, rot=-45, position=1.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIGS8f2M2Afu"
   },
   "outputs": [],
   "source": [
    "difference_df = pd.DataFrame.from_dict({\n",
    "    \"Unmitigated\": metricframe_unmitigated.difference(),\n",
    "    \"Postprocessing\": metricframe_postprocess.difference(),\n",
    "    \"Reductions\": metricframe_reductions.difference()\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "id": "3pBkP7QCDVs6",
    "outputId": "8bec1174-4a94-4cf6-8cab-634e179c0ea7"
   },
   "outputs": [],
   "source": [
    "difference_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "_qXEkByRDkK0",
    "outputId": "36ac6599-3113-4862-b5a0-7fe90bc71a5b"
   },
   "outputs": [],
   "source": [
    "difference_df.T.plot.bar(subplots=True, layout= [1,3], figsize=(12, 5), legend=False, rot=-45, position=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S6CIoJ-W1jH"
   },
   "source": [
    "### Randomized predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzdFLJ9BA15F"
   },
   "source": [
    "Both the `ExponentiatedGradient` and the `ThresholdOptimizer` yield randomized predictions (may return different result given the same instance). Due to legal regulations or other concerns, a practitioner may not be able to deploy a randomized model. To address these restrictions:\n",
    "\n",
    "*   We can create a deterministic predictor based on the randomized thresholds learned by the `ThresholdOptimizer`. This is done by interpolating a fixed threshold for each `sensitive_feature` in the `ThresholdOptimizer`.\n",
    "*   For the `ExponentiatedGradient` model, we can deploy one of the deterministic inner models rather than the overall `ExponentiatedGradient` model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9dvkQvKW1lv"
   },
   "source": [
    "### Access to sensitive features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L56tawsAW14B"
   },
   "source": [
    "\n",
    "\n",
    "*   The `ThresholdOptimizer` model requires access to the sensitive features during BOTH training time and once deployed. If you do not have access to the sensitive features once the model is deployed, you will not be able to use the `ThresholdOptimizer`.\n",
    "*   The `ExponentiatedGradient` model requires access to the sensitive features ONLY during training time. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQOR2R3XVa-U"
   },
   "source": [
    "# Model cards for model reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzFzSHrQBAgi"
   },
   "source": [
    "_Note: The Python code in this section works in Google Colab, but it may not work on all local environments._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RAXJDoyVbBT"
   },
   "source": [
    "[Mitchell et al. (2019)](https://arxiv.org/abs/1810.03993) proposed the *model cards* framework for documenting and reporting model training details and deployment considerations. A _model card_ documents, for example, training and evaluation dataset summaries, ethical considerations, and quantitative performance results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oP_tklbmMoFH"
   },
   "source": [
    "### Fill out the model card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfI9oEwIkS6v"
   },
   "source": [
    "In this section, we will create a model card for one of the models we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OViYhOAHPftT"
   },
   "outputs": [],
   "source": [
    "mct = ModelCardToolkit()\n",
    "model_card = mct.scaffold_assets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lnr6aRjIF_t6"
   },
   "source": [
    "The first section of the Model Card is the _model details_ section. In _model details_, we fill in some basic information for our model.\n",
    "\n",
    "\n",
    "*   _Name_: A name for the model\n",
    "*   _Overview_: A brief description of the model and its intended use case\n",
    "*   _Owners_: Name of individual(s) or group who created the model.\n",
    "*   _References_: Any external links or references\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKvUW25cPewp"
   },
   "outputs": [],
   "source": [
    "model_card.model_details.name = \"Diabetes Re-Admission Risk model\"\n",
    "model_card.model_details.overview = \"This model predicts whether a patient will be re-admitted into a hospital within 30 days.\"\n",
    "model_card.model_details.owners = [mctlib.Owner(\n",
    "    name=\"Fairlearn Team\",\n",
    "    contact=\"https://fairlearn.org/\")]\n",
    "model_card.model_details.references = [\n",
    "    mctlib.Reference(reference=\"https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008\")\n",
    "]\n",
    "model_card.model_details.version = mctlib.Version(name=\"v1.0\", date=str(date.today()))\n",
    "model_card.model_details.licenses = [mctlib.License(identifier=\"MIT License\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avEdkdzAPe2S"
   },
   "outputs": [],
   "source": [
    "model_card.considerations.use_cases = [mctlib.UseCase(description=\"High-Risk Patient Care Management\")]\n",
    "model_card.considerations.users = [mctlib.User(description=\"Medical Professionals\"),  mctlib.User(description=\"ML Researchers\")]\n",
    "model_card.considerations.limitations = [mctlib.Limitation(description=\n",
    "    \"\"\"\n",
    "    This model will not generalize to hospitals outside of the United States. Features, such as those encoding insurance\n",
    "    information, are inherently tied to the U.S healthcare system.\n",
    "    In addition, this model is intended for patients who are admitted into U.S. hospitals for diabetes-related illnessess.\n",
    "    \"\"\")\n",
    "]\n",
    "model_card.considerations.ethical_considerations = [mctlib.Risk(\n",
    "    name=\"Low sample sizes of certain racial groups could lead to poorer performance on these groups\",\n",
    "    mitigation_strategy=\"Collect additional data points from more hospitals.\"\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orPNYAZFwGVM"
   },
   "source": [
    "The next two sections of the model card are meant to provide the reader with information about the data used to train and evaluate the model. For each of these sections, we provide a brief `description` of the data and then submit a `visualization` of the distribution of labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khWJN_vqm35e"
   },
   "outputs": [],
   "source": [
    "model_card.model_parameters.data.append(mctlib.Dataset())\n",
    "model_card.model_parameters.data[0].graphics.description = (\n",
    "    f\"{X_train_bal.shape[0]} rows with {X_train_bal.shape[1]} features. \"\n",
    "    f\"The original training data set was undersampled to allow for an equal number of positive and negative labeled instances.\"\n",
    ")\n",
    "\n",
    "model_card.model_parameters.data[0].graphics.collection = [\n",
    "    mctlib.Graphic(name=\"Sensitive Features\", image=sensitive_train),\n",
    "    mctlib.Graphic(name=\"Target Label\", image=outcome_train)                                                  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3fiatJ6nGQf"
   },
   "outputs": [],
   "source": [
    "model_card.model_parameters.data.append(mctlib.Dataset())\n",
    "model_card.model_parameters.data[1].graphics.description = (\n",
    "    f\"{X_test.shape[0]} rows with {X_test.shape[1]} columns\"\n",
    ")\n",
    "\n",
    "model_card.model_parameters.data[1].graphics.collection = [\n",
    "    mctlib.Graphic(name=\"Sensitive Features\", image=sensitive_test),\n",
    "    mctlib.Graphic(name=\"Target Label\", image=outcome_test)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frGlrXHAyn2C"
   },
   "source": [
    "In the last section, we fill out the `quantitative_analysis` section where we describe the model's performance metrics on the evaluation dataset. In particular, we want to report the model's disagregated performance with respect to our three metrics including false negative rate, which quantifies fairness-related harms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cyHIMlAJqod"
   },
   "outputs": [],
   "source": [
    "def metricframe_to_mct_metric(mframe, feature_name):\n",
    "    \"\"\"\n",
    "    Converts a MetricFrame into a model_card_toolkit.PerformanceMetric object that can be accepted by the Model Card's\n",
    "    Quantitative Analysis section.\n",
    "    \"\"\"\n",
    "    group_metrics = mframe.by_group[feature_name].reset_index()\n",
    "    group_metrics = group_metrics.melt(id_vars=\"race\", var_name=\"type\", value_vars=feature_name).rename(columns={\"race\":\"slice\"})\n",
    "    group_metrics = group_metrics.to_dict(orient=\"records\")\n",
    "    return [mctlib.PerformanceMetric(type=d.get(\"type\"), value=str(d.get(\"value\")), slice=d.get(\"slice\")) for d in group_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kETWDtN68Gmq"
   },
   "outputs": [],
   "source": [
    "model_card.quantitative_analysis.graphics.description = (\n",
    "    f\"These graphs show the models performance on the test dataset for disagregated racial categories.\"\n",
    ")\n",
    "model_card.quantitative_analysis.performance_metrics = metricframe_to_mct_metric(metricframe_postprocess, \"false_negative_rate\")\n",
    "model_card.quantitative_analysis.graphics.collection = [\n",
    "    mctlib.Graphic(name=\"ThresholdOptimizer\", image=postprocess_performance)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvL2mHlwp-l0"
   },
   "source": [
    "Finally, we pass our filled-out `model_card` to the `mct` object to generate an HTML version of the `model_card` that can be rendered within a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-j_4mBaPe5z"
   },
   "outputs": [],
   "source": [
    "mct.update_model_card(model_card)\n",
    "html_modelcard = mct.export_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkuNE_MvMy7P"
   },
   "source": [
    "### Display the model card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "89kqC0Jj9D6O",
    "outputId": "a64294f5-205a-4dc9-8e8a-fcd245a7182f"
   },
   "outputs": [],
   "source": [
    "display.HTML(html_modelcard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1j9WzWcCVbZV"
   },
   "source": [
    "# Discussion and conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtvW54LtB8hp"
   },
   "source": [
    "In this tutorial we have explored in depth a health care scenario through all stages of the AI lifecycle except the model deployment stage. We have seen how fairness-related harms can arise at the stage of task definition, data collection, model training, and model evaluation. We have also seen how to use a variety of tools and practices, such as datasheets for datasets, Fairlearn, and model cards.\n",
    "\n",
    "Once the model is deployed, it is important to continue monitoring the key metrics to assess any performance difference as well as the potential for fairness related harms. As you learn more about how the model is used, you may need to revise the fairness metrics, update the model, consider additional sensitive features, update the task definition, or collect new data.\n",
    "\n",
    "Although we used a variety of software tools, fairness is a sociotechnical challenge, so mitigations cannot be purely technical, and need to be supported by processes and practices, including government regulation and organizational incentives.\n",
    "\n",
    "If you would like to learn more about fairness of AI systems, or to contribute to Fairlearn, we welcome you to join our community. Fairlearn is built and maintained by contributors with a variety of backgrounds and expertise.\n",
    "\n",
    "Further resources can also be found [on our website](https://fairlearn.org/main/user_guide/further_resources.html)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4MMbl3u7-J-a",
    "ZVSerRqDG3we"
   ],
   "name": "SciPy 2021 Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "a025db62d48a12d86b0e6b0cb53f59776c5e11a448915a1ba45134646da53519"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 크기 계산 함수 \n",
    "def get_gzipped_model_size(file):\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape: (25000, 200)\n",
      "y_test shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "# IMDb 데이터 다운로드 및 전처리\n",
    "max_words = 10000  # 가장 자주 등장하는 상위 10,000개 단어만 사용\n",
    "max_len = 200  # 리뷰의 최대 길이\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"my_model_RNN.h5\")\n",
    "_, accuracy = model.evaluate(\n",
    "   x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "176/176 [==============================] - 31s 163ms/step - loss: 0.1593 - accuracy: 0.9463 - val_loss: 0.3411 - val_accuracy: 0.8800\n",
      "Epoch 2/4\n",
      "176/176 [==============================] - 27s 156ms/step - loss: 0.0949 - accuracy: 0.9694 - val_loss: 0.3607 - val_accuracy: 0.8772\n",
      "Epoch 3/4\n",
      "176/176 [==============================] - 27s 156ms/step - loss: 0.0753 - accuracy: 0.9767 - val_loss: 0.3777 - val_accuracy: 0.8724\n",
      "Epoch 4/4\n",
      "176/176 [==============================] - 27s 155ms/step - loss: 0.0469 - accuracy: 0.9879 - val_loss: 0.4664 - val_accuracy: 0.8716\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_embedd  (None, 200, 128)          2560002   \n",
      " ing (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_lstm (  (None, 64)                98563     \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_  (None, 1)                 131       \n",
      " 4 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2658696 (10.14 MB)\n",
      "Trainable params: 1329473 (5.07 MB)\n",
      "Non-trainable params: 1329223 (5.07 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 가중치 가지치기 기능 실행\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "batch_size = 128\n",
    "epochs = 4\n",
    "validation_split = 0.1\n",
    "num_images = x_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                             final_sparsity=0.80,\n",
    "                                                             begin_step=0,\n",
    "                                                             end_step=end_step)\n",
    "}\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "logdir = tempfile.mkdtemp()\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "model_for_pruning.fit(x_train, y_train,\n",
    "                      batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                      callbacks=callbacks)\n",
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(x_test, y_test, verbose=0)\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create dataset (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jay\\playground\\python\\atc\\Optimizayion\\optimization_test_3.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jay/playground/python/atc/Optimizayion/optimization_test_3.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 가중치 가지치기 기능 적용 모델 저장\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Jay/playground/python/atc/Optimizayion/optimization_test_3.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_for_pruning\u001b[39m.\u001b[39;49msave(\u001b[39m\"\u001b[39;49m\u001b[39mpurned_model3.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    180\u001b[0m         parent_path, name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39mrsplit(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m    181\u001b[0m         group \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 183\u001b[0m dsid \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmake_new_dset(group, shape, dtype, data, name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    184\u001b[0m dset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mDataset(dsid)\n\u001b[0;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m dset\n",
      "File \u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\dataset.py:163\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     sid \u001b[39m=\u001b[39m h5s\u001b[39m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[1;32m--> 163\u001b[0m dset_id \u001b[39m=\u001b[39m h5d\u001b[39m.\u001b[39;49mcreate(parent\u001b[39m.\u001b[39;49mid, name, tid, sid, dcpl\u001b[39m=\u001b[39;49mdcpl, dapl\u001b[39m=\u001b[39;49mdapl)\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m (data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m    166\u001b[0m     dset_id\u001b[39m.\u001b[39mwrite(h5s\u001b[39m.\u001b[39mALL, h5s\u001b[39m.\u001b[39mALL, data)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5d.pyx:138\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create dataset (name already exists)"
     ]
    }
   ],
   "source": [
    "# 가중치 가지치기 기능 적용 모델 저장\n",
    "model_for_pruning.save(\"purned_model_RNN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create dataset (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jay\\playground\\python\\atc\\Optimizayion\\optimization_test_3.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jay/playground/python/atc/Optimizayion/optimization_test_3.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 가중치 가지치기 기능 적용 모델 저장\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Jay/playground/python/atc/Optimizayion/optimization_test_3.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_for_pruning\u001b[39m.\u001b[39;49msave(\u001b[39m\"\u001b[39;49m\u001b[39mpurned_model3.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    180\u001b[0m         parent_path, name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39mrsplit(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m    181\u001b[0m         group \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 183\u001b[0m dsid \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmake_new_dset(group, shape, dtype, data, name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    184\u001b[0m dset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mDataset(dsid)\n\u001b[0;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m dset\n",
      "File \u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\dataset.py:163\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     sid \u001b[39m=\u001b[39m h5s\u001b[39m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[1;32m--> 163\u001b[0m dset_id \u001b[39m=\u001b[39m h5d\u001b[39m.\u001b[39;49mcreate(parent\u001b[39m.\u001b[39;49mid, name, tid, sid, dcpl\u001b[39m=\u001b[39;49mdcpl, dapl\u001b[39m=\u001b[39;49mdapl)\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m (data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m    166\u001b[0m     dset_id\u001b[39m.\u001b[39mwrite(h5s\u001b[39m.\u001b[39mALL, h5s\u001b[39m.\u001b[39mALL, data)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5d.pyx:138\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create dataset (name already exists)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 869ms/step - loss: 0.5514 - accuracy: 0.9528 - val_loss: 0.5756 - val_accuracy: 0.9333\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5063 - accuracy: 0.9764 - val_loss: 0.5379 - val_accuracy: 0.9333\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4647 - accuracy: 0.9764 - val_loss: 0.5005 - val_accuracy: 0.9333\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4239 - accuracy: 0.9764 - val_loss: 0.4634 - val_accuracy: 0.9333\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_10 (Quantiz  (None, 13)                3         \n",
      " eLayer)                                                         \n",
      "                                                                 \n",
      " quant_dense (QuantizeWrapp  (None, 128)               1797      \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_dense_1 (QuantizeWra  (None, 64)                8261      \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dense_2 (QuantizeWra  (None, 32)                2085      \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dense_3 (QuantizeWra  (None, 3)                 104       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12250 (47.85 KB)\n",
      "Trainable params: 12227 (47.76 KB)\n",
      "Non-trainable params: 23 (92.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 양자화 기능 실행\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "q_aware_model = quantize_model(model_for_export)\n",
    "q_aware_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "q_aware_model.fit(x_train, y_train, batch_size=128, epochs=4, validation_split=0.1)  # 배치 크기를 원래 크기로 설정\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(x_test, y_test, verbose=0)\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# 양자화 기능 적용 모델 저장\n",
    "q_aware_model.save(\"quantized_model_RNN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기능 매트릭 정리 \n",
    "# 파일 크기를 정수로 변환하여 출력\n",
    "keras_file_size = int(get_gzipped_model_size(\"my_model_RNN.h5\"))\n",
    "pruned_keras_file_size = int(get_gzipped_model_size(\"purned_model_RNN.h5\"))\n",
    "quant_file_size = int(get_gzipped_model_size(\"quantized_model_RNN.h5\"))\n",
    "# 압축률 = (압축 전 데이터 크기) / (압축 후 데이터 크기)\n",
    "pruned_keras_compression_rate = ((keras_file_size / pruned_keras_file_size))\n",
    "quant_file_compression_rate = (keras_file_size / quant_file_size)\n",
    "pruned_keras_compression_rate = \"{:.2f}\".format(pruned_keras_compression_rate)\n",
    "quant_file_compression_rate = \"{:.2f}\".format(quant_file_compression_rate)\n",
    "# 원본 모델 비교\n",
    "PADP = (accuracy - (accuracy - model_for_pruning_accuracy)) / accuracy * 100\n",
    "if PADP >= 100:\n",
    "    PADP = 100\n",
    "P = \"{:.2f}\".format(PADP)\n",
    "PADQ = (accuracy - (accuracy - q_aware_model_accuracy)) / accuracy * 100\n",
    "if PADQ >= 100:\n",
    "    PADQ = 100\n",
    "Q = \"{:.2f}\".format(PADQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 사이즈---------------------------------------------------\n",
      "Size of gzipped baseline Keras model: 212572 bytes\n",
      "Size of gzipped pruned Keras model: 212042 bytes\n",
      "Size of gzipped Quantized model: 209090 bytes\n",
      "모델 압축률---------------------------------------------------\n",
      "compression rate of pruned Keras model:  1.00\n",
      "compression rate of Quantized model:  1.02\n",
      "모델 최적화 성능----------------------------------------------\n",
      "Baseline test accuracy: 0.9649122953414917\n",
      "Pruned test accuracy: 0.9649122953414917\n",
      "Quant test accuracy: 0.9736841917037964\n",
      "모델 최적화 비교 정확도----------------------------------------\n",
      "가지치기 비교 정확도 : 100.00 %\n",
      "양자화 비교 정확도 : 100.00 %\n"
     ]
    }
   ],
   "source": [
    "# 각 모델간(원본, 가중치 가지치기, 양자화) 성능비교 매트릭 출력\n",
    "print('모델 사이즈---------------------------------------------------')\n",
    "print(\"Size of gzipped baseline Keras model: %d bytes\" % keras_file_size)\n",
    "print(\"Size of gzipped pruned Keras model: %d bytes\" % pruned_keras_file_size)\n",
    "print(\"Size of gzipped Quantized model: %d bytes\" % quant_file_size)\n",
    "print('모델 압축률---------------------------------------------------')\n",
    "print(\"compression rate of pruned Keras model: \", pruned_keras_compression_rate)\n",
    "print(\"compression rate of Quantized model: \", quant_file_compression_rate)\n",
    "print('모델 최적화 성능----------------------------------------------')\n",
    "print('Baseline test accuracy:', accuracy)\n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)\n",
    "print('모델 최적화 비교 정확도----------------------------------------')\n",
    "print('가지치기 비교 정확도 :', P,\"%\")\n",
    "print('양자화 비교 정확도 :', Q,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

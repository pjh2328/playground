{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 및 모듈 설치\n",
    "import tempfile\n",
    "import os\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 크기 계산 함수 \n",
    "def get_gzipped_model_size(file):\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "model = tf.keras.models.load_model(\"my_model2.h5\")\n",
    "_, accuracy = model.evaluate(\n",
    "   test_images, test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 14s 33ms/step - loss: 0.9332 - accuracy: 0.6748 - val_loss: 0.9210 - val_accuracy: 0.6810\n",
      "Epoch 2/4\n",
      "352/352 [==============================] - 12s 34ms/step - loss: 0.9181 - accuracy: 0.6810 - val_loss: 1.2056 - val_accuracy: 0.5732\n",
      "Epoch 3/4\n",
      "352/352 [==============================] - 12s 34ms/step - loss: 0.8807 - accuracy: 0.6934 - val_loss: 0.8819 - val_accuracy: 0.6938\n",
      "Epoch 4/4\n",
      "352/352 [==============================] - 12s 34ms/step - loss: 0.8126 - accuracy: 0.7181 - val_loss: 0.8446 - val_accuracy: 0.7060\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d  (None, 30, 30, 32)        1762      \n",
      " _6 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_max_po  (None, 15, 15, 32)        1         \n",
      " oling2d_4 (PruneLowMagnitu                                      \n",
      " de)                                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 13, 13, 64)        36930     \n",
      " _7 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_max_po  (None, 6, 6, 64)          1         \n",
      " oling2d_5 (PruneLowMagnitu                                      \n",
      " de)                                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 4, 4, 64)          73794     \n",
      " _8 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_flatte  (None, 1024)              1         \n",
      " n_2 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_  (None, 64)                131138    \n",
      " 7 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_  (None, 10)                1292      \n",
      " 8 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244919 (956.75 KB)\n",
      "Trainable params: 122570 (478.79 KB)\n",
      "Non-trainable params: 122349 (477.96 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 가충치 가지치기 기능 실행\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "batch_size = 128\n",
    "epochs = 4\n",
    "validation_split = 0.1\n",
    "num_images = train_images.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "logdir = tempfile.mkdtemp()\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "model_for_pruning.fit(train_images, train_labels,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)\n",
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# 가중치 가지치기 기능 적용 모델 저장\n",
    "model_for_pruning.save(\"purned_model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "90/90 [==============================] - 14s 146ms/step - loss: 0.8017 - accuracy: 0.7190 - val_loss: 0.8482 - val_accuracy: 0.7074\n",
      "Epoch 2/4\n",
      "90/90 [==============================] - 13s 142ms/step - loss: 0.7687 - accuracy: 0.7322 - val_loss: 0.8466 - val_accuracy: 0.7128\n",
      "Epoch 3/4\n",
      "90/90 [==============================] - 13s 149ms/step - loss: 0.7433 - accuracy: 0.7423 - val_loss: 0.8300 - val_accuracy: 0.7152\n",
      "Epoch 4/4\n",
      "90/90 [==============================] - 14s 151ms/step - loss: 0.7261 - accuracy: 0.7486 - val_loss: 0.8119 - val_accuracy: 0.7262\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLa  (None, 32, 32, 3)         3         \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " quant_conv2d_6 (QuantizeWr  (None, 30, 30, 32)        963       \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_4 (Qua  (None, 15, 15, 32)        1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_conv2d_7 (QuantizeWr  (None, 13, 13, 64)        18627     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_5 (Qua  (None, 6, 6, 64)          1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_conv2d_8 (QuantizeWr  (None, 4, 4, 64)          37059     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_flatten_2 (QuantizeW  (None, 1024)              1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_7 (QuantizeWra  (None, 64)                65605     \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dense_8 (QuantizeWra  (None, 10)                655       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122915 (480.14 KB)\n",
      "Trainable params: 122570 (478.79 KB)\n",
      "Non-trainable params: 345 (1.35 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 양자화 기능 실행\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "q_aware_model = quantize_model(model)\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "q_aware_model.fit(train_images, train_labels,\n",
    "                  batch_size=500, epochs=4, validation_split=0.1)\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자화 기능 적용 모델 저장\n",
    "q_aware_model.save(\"quantized_model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기능 매트릭 정리 \n",
    "# 파일 크기를 정수로 변환하여 출력\n",
    "keras_file_size = int(get_gzipped_model_size(\"my_model2.h5\"))\n",
    "pruned_keras_file_size = int(get_gzipped_model_size(\"purned_model2.h5\"))\n",
    "quant_file_size = int(get_gzipped_model_size(\"quantized_model2.h5\"))\n",
    "# 압축률 = (압축 전 데이터 크기) / (압축 후 데이터 크기)\n",
    "pruned_keras_compression_rate = ((keras_file_size / pruned_keras_file_size))\n",
    "quant_file_compression_rate = (keras_file_size / quant_file_size)\n",
    "pruned_keras_compression_rate = \"{:.2f}\".format(pruned_keras_compression_rate)\n",
    "quant_file_compression_rate = \"{:.2f}\".format(quant_file_compression_rate)\n",
    "# 원본 모델 비교\n",
    "PADP = (accuracy - (accuracy - model_for_pruning_accuracy)) / accuracy * 100\n",
    "if PADP >= 100:\n",
    "    PADP = 100\n",
    "P = \"{:.2f}\".format(PADP)\n",
    "PADQ = (accuracy - (accuracy - q_aware_model_accuracy)) / accuracy * 100\n",
    "if PADQ >= 100:\n",
    "    PADQ = 100\n",
    "Q = \"{:.2f}\".format(PADQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 사이즈---------------------------------------------------\n",
      "Size of gzipped baseline Keras model: 1319277 bytes\n",
      "Size of gzipped pruned Keras model: 721579 bytes\n",
      "Size of gzipped Quantized model: 836555 bytes\n",
      "모델 압축률---------------------------------------------------\n",
      "compression rate of pruned Keras model:  1.83\n",
      "compression rate of Quantized model:  1.58\n",
      "모델 최적화 성능----------------------------------------------\n",
      "Baseline test accuracy: 0.6740000247955322\n",
      "Pruned test accuracy: 0.6980000138282776\n",
      "Quant test accuracy: 0.7024999856948853\n",
      "모델 최적화 비교 정확도----------------------------------------\n",
      "가지치기 비교 정확도 : 100.00 %\n",
      "양자화 비교 정확도 : 100.00 %\n"
     ]
    }
   ],
   "source": [
    "# 각 모델간(원본, 가중치 가지치기, 양자화) 성능비교 매트릭 출력\n",
    "print('모델 사이즈---------------------------------------------------')\n",
    "print(\"Size of gzipped baseline Keras model: %d bytes\" % keras_file_size)\n",
    "print(\"Size of gzipped pruned Keras model: %d bytes\" % pruned_keras_file_size)\n",
    "print(\"Size of gzipped Quantized model: %d bytes\" % quant_file_size)\n",
    "print('모델 압축률---------------------------------------------------')\n",
    "print(\"compression rate of pruned Keras model: \", pruned_keras_compression_rate)\n",
    "print(\"compression rate of Quantized model: \", quant_file_compression_rate)\n",
    "print('모델 최적화 성능----------------------------------------------')\n",
    "print('Baseline test accuracy:', accuracy)\n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)\n",
    "print('모델 최적화 비교 정확도----------------------------------------')\n",
    "print('가지치기 비교 정확도 :', P,\"%\")\n",
    "print('양자화 비교 정확도 :', Q,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

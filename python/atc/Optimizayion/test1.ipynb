{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2908 - accuracy: 0.9195 - val_loss: 0.1170 - val_accuracy: 0.9667\n",
      "Epoch 2/4\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.1168 - accuracy: 0.9662 - val_loss: 0.0854 - val_accuracy: 0.9767\n",
      "Epoch 3/4\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.0849 - accuracy: 0.9752 - val_loss: 0.0818 - val_accuracy: 0.9782\n",
      "Epoch 4/4\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0704 - accuracy: 0.9793 - val_loss: 0.0638 - val_accuracy: 0.9827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x19797d68fd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=4,\n",
    "  validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1688/1688 [==============================] - 8s 4ms/step - loss: 0.1287 - accuracy: 0.9655 - val_loss: 0.1001 - val_accuracy: 0.9732\n",
      "Epoch 2/4\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.1012 - accuracy: 0.9705 - val_loss: 0.0839 - val_accuracy: 0.9783\n",
      "Epoch 3/4\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0877 - accuracy: 0.9741 - val_loss: 0.0787 - val_accuracy: 0.9783\n",
      "Epoch 4/4\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0805 - accuracy: 0.9766 - val_loss: 0.0753 - val_accuracy: 0.9788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x19798728fd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가지치기를 위한 TensorFlow 모듈 불러오기\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "\n",
    "# 가지치기 모델 생성\n",
    "pruning_params = {\n",
    "    'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.2,\n",
    "                                                 final_sparsity=0.8,\n",
    "                                                 begin_step=0,\n",
    "                                                 end_step=1000,\n",
    "                                                 frequency=100)\n",
    "}\n",
    "\n",
    "# 가지치기 콜백 생성\n",
    "pruning_callbacks = [\n",
    "    sparsity.UpdatePruningStep()\n",
    "]\n",
    "\n",
    "# 가지치기 모델 컴파일\n",
    "pruned_model = sparsity.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# 가지치기 모델 훈련 및 평가\n",
    "pruned_model.compile(optimizer='adam',\n",
    "                     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "pruned_model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=4,\n",
    "  validation_split=0.1,\n",
    "  callbacks=pruning_callbacks  # 가지치기 콜백 추가\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1688/1688 [==============================] - 8s 4ms/step - loss: 0.0751 - accuracy: 0.9771 - val_loss: 0.0758 - val_accuracy: 0.9797\n",
      "Epoch 2/4\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0612 - accuracy: 0.9817 - val_loss: 0.0680 - val_accuracy: 0.9783\n",
      "Epoch 3/4\n",
      "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0538 - accuracy: 0.9836 - val_loss: 0.0631 - val_accuracy: 0.9832\n",
      "Epoch 4/4\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0490 - accuracy: 0.9848 - val_loss: 0.0619 - val_accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x197b2ecbb50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 양자화를 위한 TensorFlow 모듈 불러오기\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# 양자화 모델 생성\n",
    "quantize_model = tfmot.quantization.keras.quantize_model(model)\n",
    "\n",
    "# 양자화 모델 컴파일\n",
    "quantize_model.compile(optimizer='adam',\n",
    "                       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# 양자화 모델 훈련 및 평가\n",
    "quantize_model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=4,\n",
    "  validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import time\n",
    "import tempfile\n",
    "import zipfile  # 'zipfile' 모듈을 추가로 불러옵니다.\n",
    "\n",
    "# get_gzipped_model_size 함수 정의\n",
    "def get_gzipped_model_size(file):\n",
    "    # Returns size of gzipped model, in bytes.\n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(file)\n",
    "\n",
    "    return os.path.getsize(zipped_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 19s 11ms/step - loss: 0.1436 - accuracy: 0.9561 - val_loss: 0.0615 - val_accuracy: 0.9820\n",
      "원본 모델 평가:\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0607 - accuracy: 0.9813\n",
      "원본 모델 정확성: 0.9812999963760376\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=1,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# 원본 모델 평가\n",
    "print(\"원본 모델 평가:\")\n",
    "original_loss, original_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f\"원본 모델 정확성: {original_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 21s 11ms/step - loss: 0.0563 - accuracy: 0.9830 - val_loss: 0.0398 - val_accuracy: 0.9892\n",
      "가지치기 모델 평가:\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0366 - accuracy: 0.9882\n",
      "가지치기 모델 정확성: 0.9882000088691711\n"
     ]
    }
   ],
   "source": [
    "# 가지치기 테스트\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "\n",
    "# 가지치기 모델 생성\n",
    "pruning_params = {\n",
    "    'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.2,\n",
    "                                                 final_sparsity=0.8,\n",
    "                                                 begin_step=0,\n",
    "                                                 end_step=1000,\n",
    "                                                 frequency=100)\n",
    "}\n",
    "\n",
    "# 가지치기 콜백 생성\n",
    "pruning_callbacks = [\n",
    "    sparsity.UpdatePruningStep()\n",
    "]\n",
    "\n",
    "# 가지치기 모델 컴파일\n",
    "pruned_model = sparsity.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# 가지치기 모델 훈련 및 평가\n",
    "pruned_model.compile(optimizer='adam',\n",
    "                     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "pruned_model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=1,\n",
    "    validation_split=0.1,\n",
    "    callbacks=pruning_callbacks\n",
    ")\n",
    "\n",
    "print(\"가지치기 모델 평가:\")\n",
    "pruned_loss, pruned_accuracy = pruned_model.evaluate(test_images, test_labels)\n",
    "print(f\"가지치기 모델 정확성: {pruned_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 24s 13ms/step - loss: 0.0478 - accuracy: 0.9852 - val_loss: 0.0505 - val_accuracy: 0.9852\n",
      "양자화 모델 평가:\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0491 - accuracy: 0.9850\n",
      "양자화 모델 정확성: 0.9850000143051147\n"
     ]
    }
   ],
   "source": [
    "# 양자화 테스트\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# 양자화 모델 생성\n",
    "quantize_model = tfmot.quantization.keras.quantize_model(model)\n",
    "\n",
    "# 양자화 모델 컴파일\n",
    "quantize_model.compile(optimizer='adam',\n",
    "                       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# 양자화 모델 훈련 및 평가\n",
    "quantize_model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=1,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "print(\"양자화 모델 평가:\")\n",
    "quantized_loss, quantized_accuracy = quantize_model.evaluate(test_images, test_labels)\n",
    "print(f\"양자화 모델 정확성: {quantized_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성능 비교:\n",
      "원본 모델 정확성: 0.9812999963760376\n",
      "가지치기 모델 정확성: 0.9882000088691711\n",
      "양자화 모델 정확성: 0.9850000143051147\n",
      "원본 모델 크기: 4855 bytes\n",
      "가지치기 모델 크기: 9753 bytes\n",
      "양자화 모델 크기: 10514 bytes\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 지정된 경로를 찾을 수 없습니다: '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"module\": \"keras.layers\", \"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 28, 28], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"input_1\"}, \"registered_name\": null}, {\"module\": \"keras.layers\", \"class_name\": \"Reshape\", \"config\": {\"name\": \"reshape\", \"trainable\": true, \"dtype\": \"float32\", \"target_shape\": [28, 28, 1]}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 28, 28]}}, {\"module\": \"keras.layers\", \"class_name\": \"Conv2D\", \"config\": {\"name\": \"conv2d\", \"trainable\": true, \"dtype\": \"float32\", \"filters\": 32, \"kernel_size\": [3, 3], \"strides\": [1, 1], \"padding\": \"valid\", \"data_format\": \"channels_last\", \"dilation_rate\": [1, 1], \"groups\": 1, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 28, 28, 1]}}, {\"module\": \"keras.layers\", \"class_name\": \"MaxPooling2D\", \"config\": {\"name\": \"max_pooling2d\", \"trainable\": true, \"dtype\": \"float32\", \"pool_size\": [2, 2], \"padding\": \"valid\", \"strides\": [2, 2], \"data_format\": \"channels_last\"}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 26, 26, 32]}}, {\"module\": \"keras.layers\", \"class_name\": \"Conv2D\", \"config\": {\"name\": \"conv2d_1\", \"trainable\": true, \"dtype\": \"float32\", \"filters\": 64, \"kernel_size\": [3, 3], \"strides\": [1, 1], \"padding\": \"valid\", \"data_format\": \"channels_last\", \"dilation_rate\": [1, 1], \"groups\": 1, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 13, 13, 32]}}, {\"module\": \"keras.layers\", \"class_name\": \"MaxPooling2D\", \"config\": {\"name\": \"max_pooling2d_1\", \"trainable\": true, \"dtype\": \"float32\", \"pool_size\": [2, 2], \"padding\": \"valid\", \"strides\": [2, 2], \"data_format\": \"channels_last\"}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 11, 11, 64]}}, {\"module\": \"keras.layers\", \"class_name\": \"Flatten\", \"config\": {\"name\": \"flatten\", \"trainable\": true, \"dtype\": \"float32\", \"data_format\": \"channels_last\"}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 5, 5, 64]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 128, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 1600]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 128]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 10, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 64]}}]}, \"keras_version\": \"2.15.0\", \"backend\": \"tensorflow\"}'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m가지치기 모델 크기: \u001b[39m\u001b[39m{\u001b[39;00mpruned_model_size\u001b[39m}\u001b[39;00m\u001b[39m bytes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m양자화 모델 크기: \u001b[39m\u001b[39m{\u001b[39;00mquantized_model_size\u001b[39m}\u001b[39;00m\u001b[39m bytes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m원본 모델 압축 크기: \u001b[39m\u001b[39m{\u001b[39;00mget_gzipped_model_size(model\u001b[39m.\u001b[39;49mto_json())\u001b[39m}\u001b[39;00m\u001b[39m bytes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m가지치기 모델 압축 크기: \u001b[39m\u001b[39m{\u001b[39;00mget_gzipped_model_size(pruned_model\u001b[39m.\u001b[39mto_json())\u001b[39m}\u001b[39;00m\u001b[39m bytes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m양자화 모델 압축 크기: \u001b[39m\u001b[39m{\u001b[39;00mget_gzipped_model_size(quantize_model\u001b[39m.\u001b[39mto_json())\u001b[39m}\u001b[39;00m\u001b[39m bytes\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m, in \u001b[0;36mget_gzipped_model_size\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     10\u001b[0m _, zipped_file \u001b[39m=\u001b[39m tempfile\u001b[39m.\u001b[39mmkstemp(\u001b[39m'\u001b[39m\u001b[39m.zip\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39mZipFile(zipped_file, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, compression\u001b[39m=\u001b[39mzipfile\u001b[39m.\u001b[39mZIP_DEFLATED) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> 12\u001b[0m     f\u001b[39m.\u001b[39;49mwrite(file)\n\u001b[0;32m     14\u001b[0m \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mgetsize(zipped_file)\n",
      "File \u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1771\u001b[0m, in \u001b[0;36mZipFile.write\u001b[1;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writing:\n\u001b[0;32m   1767\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1768\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt write to ZIP archive while an open writing handle exists\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1769\u001b[0m     )\n\u001b[1;32m-> 1771\u001b[0m zinfo \u001b[39m=\u001b[39m ZipInfo\u001b[39m.\u001b[39;49mfrom_file(filename, arcname,\n\u001b[0;32m   1772\u001b[0m                           strict_timestamps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strict_timestamps)\n\u001b[0;32m   1774\u001b[0m \u001b[39mif\u001b[39;00m zinfo\u001b[39m.\u001b[39mis_dir():\n\u001b[0;32m   1775\u001b[0m     zinfo\u001b[39m.\u001b[39mcompress_size \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:528\u001b[0m, in \u001b[0;36mZipInfo.from_file\u001b[1;34m(cls, filename, arcname, strict_timestamps)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filename, os\u001b[39m.\u001b[39mPathLike):\n\u001b[0;32m    527\u001b[0m     filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(filename)\n\u001b[1;32m--> 528\u001b[0m st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(filename)\n\u001b[0;32m    529\u001b[0m isdir \u001b[39m=\u001b[39m stat\u001b[39m.\u001b[39mS_ISDIR(st\u001b[39m.\u001b[39mst_mode)\n\u001b[0;32m    530\u001b[0m mtime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mlocaltime(st\u001b[39m.\u001b[39mst_mtime)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 지정된 경로를 찾을 수 없습니다: '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"module\": \"keras.layers\", \"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 28, 28], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"input_1\"}, \"registered_name\": null}, {\"module\": \"keras.layers\", \"class_name\": \"Reshape\", \"config\": {\"name\": \"reshape\", \"trainable\": true, \"dtype\": \"float32\", \"target_shape\": [28, 28, 1]}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 28, 28]}}, {\"module\": \"keras.layers\", \"class_name\": \"Conv2D\", \"config\": {\"name\": \"conv2d\", \"trainable\": true, \"dtype\": \"float32\", \"filters\": 32, \"kernel_size\": [3, 3], \"strides\": [1, 1], \"padding\": \"valid\", \"data_format\": \"channels_last\", \"dilation_rate\": [1, 1], \"groups\": 1, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 28, 28, 1]}}, {\"module\": \"keras.layers\", \"class_name\": \"MaxPooling2D\", \"config\": {\"name\": \"max_pooling2d\", \"trainable\": true, \"dtype\": \"float32\", \"pool_size\": [2, 2], \"padding\": \"valid\", \"strides\": [2, 2], \"data_format\": \"channels_last\"}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 26, 26, 32]}}, {\"module\": \"keras.layers\", \"class_name\": \"Conv2D\", \"config\": {\"name\": \"conv2d_1\", \"trainable\": true, \"dtype\": \"float32\", \"filters\": 64, \"kernel_size\": [3, 3], \"strides\": [1, 1], \"padding\": \"valid\", \"data_format\": \"channels_last\", \"dilation_rate\": [1, 1], \"groups\": 1, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 13, 13, 32]}}, {\"module\": \"keras.layers\", \"class_name\": \"MaxPooling2D\", \"config\": {\"name\": \"max_pooling2d_1\", \"trainable\": true, \"dtype\": \"float32\", \"pool_size\": [2, 2], \"padding\": \"valid\", \"strides\": [2, 2], \"data_format\": \"channels_last\"}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 11, 11, 64]}}, {\"module\": \"keras.layers\", \"class_name\": \"Flatten\", \"config\": {\"name\": \"flatten\", \"trainable\": true, \"dtype\": \"float32\", \"data_format\": \"channels_last\"}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 5, 5, 64]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 128, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 1600]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 128]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 10, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 64]}}]}, \"keras_version\": \"2.15.0\", \"backend\": \"tensorflow\"}'"
     ]
    }
   ],
   "source": [
    "# 성능 비교\n",
    "print(\"성능 비교:\")\n",
    "print(f\"원본 모델 정확성: {original_accuracy}\")\n",
    "print(f\"가지치기 모델 정확성: {pruned_accuracy}\")\n",
    "print(f\"양자화 모델 정확성: {quantized_accuracy}\")\n",
    "\n",
    "# 모델 크기 비교\n",
    "original_model_size = len(model.to_json())\n",
    "pruned_model_size = len(pruned_model.to_json())\n",
    "quantized_model_size = len(quantize_model.to_json())\n",
    "\n",
    "print(f\"원본 모델 크기: {original_model_size} bytes\")\n",
    "print(f\"가지치기 모델 크기: {pruned_model_size} bytes\")\n",
    "print(f\"양자화 모델 크기: {quantized_model_size} bytes\")\n",
    "\n",
    "print(f\"원본 모델 압축 크기: {get_gzipped_model_size(model.to_json())} bytes\")\n",
    "print(f\"가지치기 모델 압축 크기: {get_gzipped_model_size(pruned_model.to_json())} bytes\")\n",
    "print(f\"양자화 모델 압축 크기: {get_gzipped_model_size(quantize_model.to_json())} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 18s 10ms/step - loss: 0.1507 - accuracy: 0.9533 - val_loss: 0.0501 - val_accuracy: 0.9857\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 2.3004 - accuracy: 0.1135\n",
      "가지치기 모델 평가 결과:\n",
      "손실: 2.3004472255706787\n",
      "정확성: 0.11349999904632568\n",
      "추론 시간: 1.5863237380981445 초\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 2.3033 - accuracy: 0.1135\n",
      "양자화 모델 평가 결과:\n",
      "손실: 2.3033483028411865\n",
      "정확성: 0.11349999904632568\n",
      "추론 시간: 1.9169261455535889 초\n",
      "성능 비교:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2066, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2049, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2037, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1917, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'reshape_4' (type Reshape).\n    \n    as_list() is not defined on an unknown TensorShape.\n    \n    Call arguments received by layer 'reshape_4' (type Reshape):\n      • inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 123\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39m# 성능 비교\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m성능 비교:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 123\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m원본 모델 정확성:\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m.\u001b[39;49mevaluate(test_images, test_labels)[\u001b[39m1\u001b[39m])\n\u001b[0;32m    124\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m가지치기 모델 정확성:\u001b[39m\u001b[39m\"\u001b[39m, pruned_accuracy)\n\u001b[0;32m    125\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m양자화 모델 정확성:\u001b[39m\u001b[39m\"\u001b[39m, quantized_accuracy)\n",
      "File \u001b[1;32mc:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filex8f8hocv.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2066, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2049, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2037, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1917, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'reshape_4' (type Reshape).\n    \n    as_list() is not defined on an unknown TensorShape.\n    \n    Call arguments received by layer 'reshape_4' (type Reshape):\n      • inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import time\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# get_gzipped_model_size 함수 정의\n",
    "def get_gzipped_model_size(model):\n",
    "    _, keras_file = tempfile.mkstemp('.h5')\n",
    "    tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "    \n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(keras_file)\n",
    "    \n",
    "    return os.path.getsize(zipped_file)\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=1,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"my_model.h5\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model(\"my_model.h5\")\n",
    "\n",
    "# 가지치기 함수 정의\n",
    "def apply_pruning_to_loaded_model(loaded_model):\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=0.2,\n",
    "            final_sparsity=0.8,\n",
    "            begin_step=0,\n",
    "            end_step=1000,\n",
    "            frequency=100\n",
    "        )\n",
    "    }\n",
    "\n",
    "    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(loaded_model, **pruning_params)\n",
    "    pruned_model.compile(optimizer='adam',\n",
    "                         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "# 가지치기 모델 생성\n",
    "pruned_loaded_model = apply_pruning_to_loaded_model(loaded_model)\n",
    "\n",
    "# 가지치기 모델 평가\n",
    "def evaluate_model(model, test_images, test_labels):\n",
    "    start_time = time.time()\n",
    "    loss, accuracy = model.evaluate(test_images, test_labels)\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    return loss, accuracy, inference_time\n",
    "\n",
    "# 테스트 데이터 준비\n",
    "test_images = test_images.reshape((-1, 28, 28, 1))\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# 가지치기 모델 평가\n",
    "pruned_loss, pruned_accuracy, pruned_inference_time = evaluate_model(pruned_loaded_model, test_images, test_labels)\n",
    "\n",
    "print(\"가지치기 모델 평가 결과:\")\n",
    "print(f\"손실: {pruned_loss}\")\n",
    "print(f\"정확성: {pruned_accuracy}\")\n",
    "print(f\"추론 시간: {pruned_inference_time} 초\")\n",
    "\n",
    "# 양자화 함수 정의\n",
    "def apply_quantization_to_loaded_model(loaded_model):\n",
    "    quantize_model = tfmot.quantization.keras.quantize_model(loaded_model)\n",
    "    quantize_model.compile(optimizer='adam',\n",
    "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                           metrics=['accuracy'])\n",
    "    return quantize_model\n",
    "\n",
    "# 양자화 모델 생성\n",
    "quantized_loaded_model = apply_quantization_to_loaded_model(loaded_model)\n",
    "\n",
    "# 양자화 모델 평가\n",
    "quantized_loss, quantized_accuracy, quantized_inference_time = evaluate_model(quantized_loaded_model, test_images, test_labels)\n",
    "\n",
    "print(\"양자화 모델 평가 결과:\")\n",
    "print(f\"손실: {quantized_loss}\")\n",
    "print(f\"정확성: {quantized_accuracy}\")\n",
    "print(f\"추론 시간: {quantized_inference_time} 초\")\n",
    "\n",
    "# 성능 비교\n",
    "print(\"성능 비교:\")\n",
    "print(\"원본 모델 정확성:\", model.evaluate(test_images, test_labels)[1])\n",
    "print(\"가지치기 모델 정확성:\", pruned_accuracy)\n",
    "print(\"양자화 모델 정확성:\", quantized_accuracy)\n",
    "\n",
    "# 모델 크기 비교\n",
    "original_model_size = len(loaded_model.to_json())\n",
    "pruned_model_size = len(pruned_loaded_model.to_json())\n",
    "quantized_model_size = len(quantized_loaded_model.to_json())\n",
    "\n",
    "print(\"원본 모델 크기:\", original_model_size, \"bytes\")\n",
    "print(\"가지치기 모델 크기:\", pruned_model_size, \"bytes\")\n",
    "print(\"양자화 모델 크기:\", quantized_model_size, \"bytes\")\n",
    "\n",
    "# 압축된 모델 크기 비교\n",
    "original_gzipped_model_size = get_gzipped_model_size(loaded_model)\n",
    "pruned_gzipped_model_size = get_gzipped_model_size(pruned_loaded_model)\n",
    "quantized_gzipped_model_size = get_gzipped_model_size(quantized_loaded_model)\n",
    "\n",
    "print(\"원본 모델 압축 크기:\", original_gzipped_model_size, \"bytes\")\n",
    "print(\"가지치기 모델 압축 크기:\", pruned_gzipped_model_size, \"bytes\")\n",
    "print(\"양자화 모델 압축 크기:\", quantized_gzipped_model_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 18s 10ms/step - loss: 0.1558 - accuracy: 0.9514 - val_loss: 0.0567 - val_accuracy: 0.9833\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0531 - accuracy: 0.9823\n",
      "가지치기 모델 정확성: 0.9822999835014343\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 2.3015 - accuracy: 0.1135\n",
      "양자화 모델 정확성: 0.11349999904632568\n",
      "성능 비교:\n",
      "원본 모델 정확성: 0.9812999963760376\n",
      "가지치기 모델 정확성: 0.9822999835014343\n",
      "양자화 모델 정확성: 0.11349999904632568\n",
      "원본 모델 크기: 4874 bytes\n",
      "가지치기 모델 크기: 9789 bytes\n",
      "양자화 모델 크기: 10552 bytes\n",
      "원본 모델 압축 크기: 865366 bytes\n",
      "가지치기 모델 압축 크기: 870030 bytes\n",
      "양자화 모델 압축 크기: 867916 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jay\\AppData\\Local\\Temp\\ipykernel_21552\\1632045003.py:12: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import time\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# get_gzipped_model_size 함수 정의\n",
    "def get_gzipped_model_size(model):\n",
    "    _, keras_file = tempfile.mkstemp('.h5')\n",
    "    tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "    \n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(keras_file)\n",
    "    \n",
    "    return os.path.getsize(zipped_file)\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=1,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"my_model.h5\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model(\"my_model.h5\")\n",
    "\n",
    "# 가지치기 함수 정의\n",
    "def apply_pruning_to_loaded_model(loaded_model):\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=0.2,\n",
    "            final_sparsity=0.8,\n",
    "            begin_step=0,\n",
    "            end_step=1000,\n",
    "            frequency=100\n",
    "        )\n",
    "    }\n",
    "\n",
    "    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(loaded_model, **pruning_params)\n",
    "    pruned_model.compile(optimizer='adam',\n",
    "                         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "# 가지치기 모델 생성\n",
    "pruned_loaded_model = apply_pruning_to_loaded_model(loaded_model)\n",
    "\n",
    "# 가지치기 모델 평가\n",
    "_, pruned_accuracy = pruned_loaded_model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"가지치기 모델 정확성:\", pruned_accuracy)\n",
    "\n",
    "# 양자화 함수 정의\n",
    "def apply_quantization_to_loaded_model(loaded_model):\n",
    "    quantize_model = tfmot.quantization.keras.quantize_model(loaded_model)\n",
    "    quantize_model.compile(optimizer='adam',\n",
    "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                           metrics=['accuracy'])\n",
    "    return quantize_model\n",
    "\n",
    "# 양자화 모델 생성\n",
    "quantized_loaded_model = apply_quantization_to_loaded_model(loaded_model)\n",
    "\n",
    "# 양자화 모델 평가\n",
    "_, quantized_accuracy = quantized_loaded_model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"양자화 모델 정확성:\", quantized_accuracy)\n",
    "\n",
    "# 성능 비교\n",
    "print(\"성능 비교:\")\n",
    "print(\"원본 모델 정확성:\", original_accuracy)\n",
    "print(\"가지치기 모델 정확성:\", pruned_accuracy)\n",
    "print(\"양자화 모델 정확성:\", quantized_accuracy)\n",
    "\n",
    "# 모델 크기 비교\n",
    "original_model_size = len(loaded_model.to_json())\n",
    "pruned_model_size = len(pruned_loaded_model.to_json())\n",
    "quantized_model_size = len(quantized_loaded_model.to_json())\n",
    "\n",
    "print(\"원본 모델 크기:\", original_model_size, \"bytes\")\n",
    "print(\"가지치기 모델 크기:\", pruned_model_size, \"bytes\")\n",
    "print(\"양자화 모델 크기:\", quantized_model_size, \"bytes\")\n",
    "\n",
    "# 압축된 모델 크기 비교\n",
    "original_gzipped_model_size = get_gzipped_model_size(loaded_model)\n",
    "pruned_gzipped_model_size = get_gzipped_model_size(pruned_loaded_model)\n",
    "quantized_gzipped_model_size = get_gzipped_model_size(quantized_loaded_model)\n",
    "\n",
    "print(\"원본 모델 압축 크기:\", original_gzipped_model_size, \"bytes\")\n",
    "print(\"가지치기 모델 압축 크기:\", pruned_gzipped_model_size, \"bytes\")\n",
    "print(\"양자화 모델 압축 크기:\", quantized_gzipped_model_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 19s 11ms/step - loss: 0.1510 - accuracy: 0.9534 - val_loss: 0.0557 - val_accuracy: 0.9828\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.0569 - accuracy: 0.9806\n",
      "가지치기 모델 정확성: 0.9805999994277954\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 2.3145 - accuracy: 0.1135\n",
      "양자화 모델 정확성: 0.11349999904632568\n",
      "성능 비교:\n",
      "원본 모델 정확성: 0.9812999963760376\n",
      "가지치기 모델 정확성: 0.9805999994277954\n",
      "양자화 모델 정확성: 0.11349999904632568\n",
      "원본 모델 크기: 4874 bytes\n",
      "가지치기 모델 크기: 9789 bytes\n",
      "양자화 모델 크기: 10552 bytes\n",
      "원본 모델 압축 크기: 865860 bytes\n",
      "가지치기 모델 압축 크기: 870478 bytes\n",
      "양자화 모델 압축 크기: 868354 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jay\\AppData\\Local\\Temp\\ipykernel_21552\\2276957639.py:12: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import time\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# get_gzipped_model_size 함수 정의\n",
    "def get_gzipped_model_size(model):\n",
    "    _, keras_file = tempfile.mkstemp('.h5')\n",
    "    tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "    \n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(keras_file)\n",
    "    \n",
    "    return os.path.getsize(zipped_file)\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=1,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Save the model in HDF5 format\n",
    "model.save(\"my_model.h5\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model(\"my_model.h5\")\n",
    "\n",
    "# 가지치기 함수 정의\n",
    "def apply_pruning_to_loaded_model(loaded_model):\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=0.2,\n",
    "            final_sparsity=0.8,\n",
    "            begin_step=0,\n",
    "            end_step=1000,\n",
    "            frequency=100\n",
    "        )\n",
    "    }\n",
    "\n",
    "    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(loaded_model, **pruning_params)\n",
    "    pruned_model.compile(optimizer='adam',\n",
    "                         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "# 가지치기 모델 생성\n",
    "pruned_model = apply_pruning_to_loaded_model(loaded_model)\n",
    "\n",
    "# 가지치기 모델 평가\n",
    "_, pruned_accuracy = pruned_model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"가지치기 모델 정확성:\", pruned_accuracy)\n",
    "\n",
    "# 양자화 함수 정의\n",
    "def apply_quantization_to_loaded_model(loaded_model):\n",
    "    quantize_model = tfmot.quantization.keras.quantize_model(loaded_model)\n",
    "    quantize_model.compile(optimizer='adam',\n",
    "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                           metrics=['accuracy'])\n",
    "    return quantize_model\n",
    "\n",
    "# 양자화 모델 생성\n",
    "quantized_model = apply_quantization_to_loaded_model(loaded_model)\n",
    "\n",
    "# 양자화 모델 평가\n",
    "_, quantized_accuracy = quantized_model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"양자화 모델 정확성:\", quantized_accuracy)\n",
    "\n",
    "# 성능 비교\n",
    "print(\"성능 비교:\")\n",
    "print(\"원본 모델 정확성:\", original_accuracy)\n",
    "print(\"가지치기 모델 정확성:\", pruned_accuracy)\n",
    "print(\"양자화 모델 정확성:\", quantized_accuracy)\n",
    "\n",
    "# 모델 크기 비교\n",
    "original_model_size = len(loaded_model.to_json())\n",
    "pruned_model_size = len(pruned_model.to_json())\n",
    "quantized_model_size = len(quantized_model.to_json())\n",
    "\n",
    "print(\"원본 모델 크기:\", original_model_size, \"bytes\")\n",
    "print(\"가지치기 모델 크기:\", pruned_model_size, \"bytes\")\n",
    "print(\"양자화 모델 크기:\", quantized_model_size, \"bytes\")\n",
    "\n",
    "# 압축된 모델 크기 비교\n",
    "original_gzipped_model_size = get_gzipped_model_size(loaded_model)\n",
    "pruned_gzipped_model_size = get_gzipped_model_size(pruned_model)\n",
    "quantized_gzipped_model_size = get_gzipped_model_size(quantized_model)\n",
    "\n",
    "print(\"원본 모델 압축 크기:\", original_gzipped_model_size, \"bytes\")\n",
    "print(\"가지치기 모델 압축 크기:\", pruned_gzipped_model_size, \"bytes\")\n",
    "print(\"양자화 모델 압축 크기:\", quantized_gzipped_model_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 19s 11ms/step - loss: 0.3949 - accuracy: 0.8779 - val_loss: 0.0534 - val_accuracy: 0.9842\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.0549 - accuracy: 0.9819\n",
      "가지치기 모델 정확성: 0.9818999767303467\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 2.3066 - accuracy: 0.0974\n",
      "양자화 모델 정확성: 0.09740000218153\n",
      "성능 비교:\n",
      "원본 모델 정확성: 0.9812999963760376\n",
      "가지치기 모델 정확성: 0.9818999767303467\n",
      "양자화 모델 정확성: 0.09740000218153\n",
      "원본 모델 크기: 5363 bytes\n",
      "가지치기 모델 크기: 11359 bytes\n",
      "양자화 모델 크기: 11934 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jay\\AppData\\Local\\Temp\\ipykernel_21552\\79366680.py:12: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 모델 압축 크기: 865923 bytes\n",
      "가지치기 모델 압축 크기: 870878 bytes\n",
      "양자화 모델 압축 크기: 868652 bytes\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import time\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# get_gzipped_model_size 함수 정의\n",
    "def get_gzipped_model_size(model):\n",
    "    _, keras_file = tempfile.mkstemp('.h5')\n",
    "    tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "    \n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(keras_file)\n",
    "    \n",
    "    return os.path.getsize(zipped_file)\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define a more complex CNN model architecture.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),  # 드롭아웃 레이어 추가\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),  # 드롭아웃 레이어 추가\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=1,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Save the model in HDF5 format\n",
    "model.save(\"my_model.h5\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model(\"my_model.h5\")\n",
    "\n",
    "# 가지치기 함수 정의\n",
    "def apply_pruning_to_loaded_model(loaded_model):\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=0.2,\n",
    "            final_sparsity=0.8,\n",
    "            begin_step=0,\n",
    "            end_step=1000,\n",
    "            frequency=100\n",
    "        )\n",
    "    }\n",
    "\n",
    "    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(loaded_model, **pruning_params)\n",
    "    pruned_model.compile(optimizer='adam',\n",
    "                         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "# 가지치기 모델 생성\n",
    "pruned_model = apply_pruning_to_loaded_model(loaded_model)\n",
    "\n",
    "# 가지치기 모델 평가\n",
    "_, pruned_accuracy = pruned_model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"가지치기 모델 정확성:\", pruned_accuracy)\n",
    "\n",
    "# 양자화 함수 정의\n",
    "def apply_quantization_to_loaded_model(loaded_model):\n",
    "    quantize_model = tfmot.quantization.keras.quantize_model(loaded_model)\n",
    "    quantize_model.compile(optimizer='adam',\n",
    "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                           metrics=['accuracy'])\n",
    "    return quantize_model\n",
    "\n",
    "# 양자화 모델 생성\n",
    "quantized_model = apply_quantization_to_loaded_model(loaded_model)\n",
    "\n",
    "# 양자화 모델 평가\n",
    "_, quantized_accuracy = quantized_model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"양자화 모델 정확성:\", quantized_accuracy)\n",
    "\n",
    "# 성능 비교\n",
    "print(\"성능 비교:\")\n",
    "print(\"원본 모델 정확성:\", original_accuracy)\n",
    "print(\"가지치기 모델 정확성:\", pruned_accuracy)\n",
    "print(\"양자화 모델 정확성:\", quantized_accuracy)\n",
    "\n",
    "# 모델 크기 비교\n",
    "original_model_size = len(loaded_model.to_json())\n",
    "pruned_model_size = len(pruned_model.to_json())\n",
    "quantized_model_size = len(quantized_model.to_json())\n",
    "\n",
    "print(\"원본 모델 크기:\", original_model_size, \"bytes\")\n",
    "print(\"가지치기 모델 크기:\", pruned_model_size, \"bytes\")\n",
    "print(\"양자화 모델 크기:\", quantized_model_size, \"bytes\")\n",
    "\n",
    "# 압축된 모델 크기 비교\n",
    "original_gzipped_model_size = get_gzipped_model_size(loaded_model)\n",
    "pruned_gzipped_model_size = get_gzipped_model_size(pruned_model)\n",
    "quantized_gzipped_model_size = get_gzipped_model_size(quantized_model)\n",
    "\n",
    "print(\"원본 모델 압축 크기:\", original_gzipped_model_size, \"bytes\")\n",
    "print(\"가지치기 모델 압축 크기:\", pruned_gzipped_model_size, \"bytes\")\n",
    "print(\"양자화 모델 압축 크기:\", quantized_gzipped_model_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2:\n",
      "Original Shape: (3, 3, 1, 32)\n",
      "Pruned Shape: (3, 3, 1, 32)\n",
      "Quantized Shape: ()\n",
      "Weight Difference (Original vs. Pruned): 0.0\n",
      "Weight Difference (Original vs. Quantized): 1.011624813079834\n",
      "\n",
      "\n",
      "Layer 4:\n",
      "Original Shape: (3, 3, 32, 64)\n",
      "Pruned Shape: (3, 3, 32, 64)\n",
      "Quantized Shape: ()\n",
      "Weight Difference (Original vs. Pruned): 0.0\n",
      "Weight Difference (Original vs. Quantized): 0.9835658669471741\n",
      "\n",
      "\n",
      "Layer 7:\n",
      "Original Shape: (1600, 128)\n",
      "Pruned Shape: (1600, 128)\n",
      "Quantized Shape: ()\n",
      "Weight Difference (Original vs. Pruned): 0.0\n",
      "Weight Difference (Original vs. Quantized): 0.9931374192237854\n",
      "\n",
      "\n",
      "Layer 9:\n",
      "Original Shape: (128, 64)\n",
      "Pruned Shape: (128, 64)\n",
      "Quantized Shape: ()\n",
      "Weight Difference (Original vs. Pruned): 0.0\n",
      "Weight Difference (Original vs. Quantized): 1.002368688583374\n",
      "\n",
      "\n",
      "Layer 11:\n",
      "Original Shape: (64, 10)\n",
      "Pruned Shape: (64, 10)\n",
      "Quantized Shape: ()\n",
      "Weight Difference (Original vs. Pruned): 0.0\n",
      "Weight Difference (Original vs. Quantized): 0.9837929010391235\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 원본 모델의 가중치\n",
    "original_weights = [layer.get_weights() for layer in loaded_model.layers]\n",
    "\n",
    "# 가지치기 모델의 가중치\n",
    "pruned_weights = [layer.get_weights() for layer in pruned_model.layers]\n",
    "\n",
    "# 양자화 모델의 가중치\n",
    "quantized_weights = [layer.get_weights() for layer in quantized_model.layers]\n",
    "\n",
    "# 레이어별로 가중치가 어떻게 바뀌었는지 확인\n",
    "for i, (original_layer, pruned_layer, quantized_layer) in enumerate(zip(original_weights, pruned_weights, quantized_weights)):\n",
    "    if len(original_layer) > 0:  # 가중치가 있는 레이어만 비교\n",
    "        original_shape = np.array(original_layer[0]).shape\n",
    "        pruned_shape = np.array(pruned_layer[0]).shape\n",
    "        quantized_shape = np.array(quantized_layer[0]).shape\n",
    "\n",
    "        print(f\"Layer {i+1}:\")\n",
    "        print(f\"Original Shape: {original_shape}\")\n",
    "        print(f\"Pruned Shape: {pruned_shape}\")\n",
    "        print(f\"Quantized Shape: {quantized_shape}\")\n",
    "\n",
    "        # 원본 가중치와 가지치기 모델, 양자화 모델 간의 차이 계산\n",
    "        weight_diff_pruned = np.abs(np.array(original_layer[0]) - np.array(pruned_layer[0]))\n",
    "        weight_diff_quantized = np.abs(np.array(original_layer[0]) - np.array(quantized_layer[0]))\n",
    "\n",
    "        print(f\"Weight Difference (Original vs. Pruned): {np.mean(weight_diff_pruned)}\")\n",
    "        print(f\"Weight Difference (Original vs. Quantized): {np.mean(weight_diff_quantized)}\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import time\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# get_gzipped_model_size 함수 정의\n",
    "def get_gzipped_model_size(model):\n",
    "    _, keras_file = tempfile.mkstemp('.h5')\n",
    "    tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "    \n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(keras_file)\n",
    "    \n",
    "    return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 18s 10ms/step - loss: 0.1450 - accuracy: 0.9562 - val_loss: 0.0501 - val_accuracy: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1403537a010>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=1,\n",
    "    validation_split=0.1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가지치기 함수 정의\n",
    "def apply_pruning_to_loaded_model(loaded_model):\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=0.2,\n",
    "            final_sparsity=0.8,\n",
    "            begin_step=0,\n",
    "            end_step=1000,\n",
    "            frequency=100\n",
    "        )\n",
    "    }\n",
    "\n",
    "    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(loaded_model, **pruning_params)\n",
    "    pruned_model.compile(optimizer='adam',\n",
    "                         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "# 가지치기 모델 생성\n",
    "loaded_model = keras.models.load_model(\"my_model.h5\")\n",
    "pruned_model = apply_pruning_to_loaded_model(loaded_model)\n",
    "\n",
    "# 가지치기 모델의 가지치기 제거\n",
    "final_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Jay\\AppData\\Local\\Temp\\tmpb_ezif9i\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Jay\\AppData\\Local\\Temp\\tmpb_ezif9i\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 2.3066 - accuracy: 0.0974\n",
      "양자화 모델 정확성: 0.09740000218153\n"
     ]
    }
   ],
   "source": [
    "# 양자화 함수 정의\n",
    "def apply_quantization_to_loaded_model(loaded_model):\n",
    "    quantize_model = tfmot.quantization.keras.quantize_model(loaded_model)\n",
    "    quantize_model.compile(optimizer='adam',\n",
    "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                           metrics=['accuracy'])\n",
    "    return quantize_model\n",
    "\n",
    "# 양자화 모델 생성\n",
    "quantized_model = apply_quantization_to_loaded_model(final_pruned_model)\n",
    "\n",
    "# TFLite 변환 (가지치기 모델을 양자화 모델로 변환)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quantized_model)\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "# TFLite 모델 저장\n",
    "with open(\"quantized_model.tflite\", \"wb\") as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "\n",
    "# 양자화 모델 평가\n",
    "_, quantized_accuracy = quantized_model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"양자화 모델 정확성:\", quantized_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성능 비교:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'original_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 성능 비교\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m성능 비교:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m원본 모델 정확성:\u001b[39m\u001b[39m\"\u001b[39m, original_accuracy)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m가지치기 모델 정확성:\u001b[39m\u001b[39m\"\u001b[39m, pruned_accuracy)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m양자화 모델 정확성:\u001b[39m\u001b[39m\"\u001b[39m, quantized_accuracy)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'original_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "# 성능 비교\n",
    "print(\"성능 비교:\")\n",
    "print(\"원본 모델 정확성:\", original_accuracy)\n",
    "print(\"가지치기 모델 정확성:\", pruned_accuracy)\n",
    "print(\"양자화 모델 정확성:\", quantized_accuracy)\n",
    "\n",
    "# 모델 크기 비교\n",
    "original_model_size = len(loaded_model.to_json())\n",
    "pruned_model_size = len(pruned_model.to_json())\n",
    "quantized_model_size = len(quantized_model.to_json())\n",
    "\n",
    "print(\"원본 모델 크기:\", original_model_size, \"bytes\")\n",
    "print(\"가지치기 모델 크기:\", pruned_model_size, \"bytes\")\n",
    "print(\"양자화 모델 크기:\", quantized_model_size, \"bytes\")\n",
    "\n",
    "# 압축된 모델 크기 비교\n",
    "original_gzipped_model_size = get_gzipped_model_size(loaded_model)\n",
    "pruned_gzipped_model_size = get_gzipped_model_size(pruned_model)\n",
    "quantized_gzipped_model_size = get_gzipped_model_size(quantized_model)\n",
    "\n",
    "print(\"원본 모델 압축 크기:\", original_gzipped_model_size, \"bytes\")\n",
    "print(\"가지치기 모델 압축 크기:\", pruned_gzipped_model_size, \"bytes\")\n",
    "print(\"양자화 모델 압축 크기:\", quantized_gzipped_model_size, \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 18s 10ms/step - loss: 0.1435 - accuracy: 0.9553 - val_loss: 0.0474 - val_accuracy: 0.9868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Jay\\AppData\\Local\\Temp\\tmpd4qcyjz9\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Jay\\AppData\\Local\\Temp\\tmpd4qcyjz9\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 2.3038 - accuracy: 0.1135\n",
      "양자화 모델 정확성: 0.11349999904632568\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0517 - accuracy: 0.9837\n",
      "원본 모델 정확성: 0.9836999773979187\n",
      "성능 비교:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'final_pruned_model_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 117\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m# 성능 비교\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m성능 비교:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 117\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m가지치기 모델 정확성:\u001b[39m\u001b[39m\"\u001b[39m, final_pruned_model_accuracy)\n\u001b[0;32m    118\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m양자화 모델 정확성:\u001b[39m\u001b[39m\"\u001b[39m, quantized_accuracy)\n\u001b[0;32m    120\u001b[0m \u001b[39m# 모델 크기 비교\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_pruned_model_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import time\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# get_gzipped_model_size 함수 정의\n",
    "def get_gzipped_model_size(model):\n",
    "    _, keras_file = tempfile.mkstemp('.h5')\n",
    "    tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "    \n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(keras_file)\n",
    "    \n",
    "    return os.path.getsize(zipped_file)\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=1,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Save the model in HDF5 format\n",
    "model.save(\"my_model.h5\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model(\"my_model.h5\")\n",
    "\n",
    "# 가지치기 함수 정의\n",
    "def apply_pruning_to_loaded_model(loaded_model):\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=0.2,\n",
    "            final_sparsity=0.8,\n",
    "            begin_step=0,\n",
    "            end_step=1000,\n",
    "            frequency=100\n",
    "        )\n",
    "    }\n",
    "\n",
    "    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(loaded_model, **pruning_params)\n",
    "    pruned_model.compile(optimizer='adam',\n",
    "                         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "# 가지치기 모델 생성\n",
    "loaded_model = keras.models.load_model(\"my_model.h5\")\n",
    "pruned_model = apply_pruning_to_loaded_model(loaded_model)\n",
    "\n",
    "# 가지치기 모델의 가지치기 제거\n",
    "final_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "# 양자화 함수 정의\n",
    "def apply_quantization_to_loaded_model(loaded_model):\n",
    "    quantize_model = tfmot.quantization.keras.quantize_model(loaded_model)\n",
    "    quantize_model.compile(optimizer='adam',\n",
    "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                           metrics=['accuracy'])\n",
    "    return quantize_model\n",
    "\n",
    "# 양자화 모델 생성\n",
    "quantized_model = apply_quantization_to_loaded_model(final_pruned_model)\n",
    "\n",
    "# TFLite 변환 (가지치기 모델을 양자화 모델로 변환)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quantized_model)\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "# TFLite 모델 저장\n",
    "with open(\"quantized_model.tflite\", \"wb\") as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "\n",
    "# 양자화 모델 평가\n",
    "_, quantized_accuracy = quantized_model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"양자화 모델 정확성:\", quantized_accuracy)\n",
    "\n",
    "# 원본 모델 평가\n",
    "_, original_accuracy = loaded_model.evaluate(test_images, test_labels)\n",
    "print(\"원본 모델 정확성:\", original_accuracy)\n",
    "\n",
    "# 성능 비교\n",
    "print(\"성능 비교:\")\n",
    "print(\"가지치기 모델 정확성:\", final_pruned_model_accuracy)\n",
    "print(\"양자화 모델 정확성:\", quantized_accuracy)\n",
    "\n",
    "# 모델 크기 비교\n",
    "original_model_size = len(loaded_model.to_json())\n",
    "pruned_model_size = len(final_pruned_model.to_json())\n",
    "quantized_model_size = len(quantized_model.to_json())\n",
    "\n",
    "print(\"원본 모델 크기:\", original_model_size, \"bytes\")\n",
    "print(\"가지치기 모델 크기:\", pruned_model_size, \"bytes\")\n",
    "print(\"양자화 모델 크기:\", quantized_model_size, \"bytes\")\n",
    "\n",
    "# 압축된 모델 크기 비교\n",
    "original_gzipped_model_size = get_gzipped_model_size(loaded_model)\n",
    "pruned_gzipped_model_size = get_gzipped_model_size(final_pruned_model)\n",
    "quantized_gzipped_model_size = get_gzipped_model_size(quantized_model)\n",
    "\n",
    "print(\"원본 모델 압축 크기:\", original_gzipped_model_size, \"bytes\")\n",
    "print(\"가지치기 모델 압축 크기:\", pruned_gzipped_model_size, \"bytes\")\n",
    "print(\"양자화 모델 압축 크기:\", quantized_gzipped_model_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 16s 9ms/step - loss: 0.1563 - accuracy: 0.9521 - val_loss: 0.0631 - val_accuracy: 0.9803\n",
      "Epoch 1/2\n",
      "422/422 [==============================] - 13s 26ms/step - loss: 0.0584 - accuracy: 0.9827 - val_loss: 0.0523 - val_accuracy: 0.9838\n",
      "Epoch 2/2\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0410 - accuracy: 0.9880 - val_loss: 0.0347 - val_accuracy: 0.9898\n",
      "Pruned test accuracy: 0.9891999959945679\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jay\\AppData\\Local\\Temp\\ipykernel_31760\\2195050560.py:107: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  tf.keras.models.save_model(final_pruned_model, pruned_keras_file, include_optimizer=False)\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned Keras model to: C:\\Users\\Jay\\AppData\\Local\\Temp\\tmpyxnpoz4a.h5\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Jay\\AppData\\Local\\Temp\\tmphazu83x6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Jay\\AppData\\Local\\Temp\\tmphazu83x6\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step - loss: 2.3277 - accuracy: 0.1135\n",
      "양자화 모델 정확성: 0.11349999904632568\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0339 - accuracy: 0.9892\n",
      "원본 모델 정확성: 0.9891999959945679\n",
      "성능 비교:\n",
      "가지치기 모델 정확성: 0.9891999959945679\n",
      "양자화 모델 정확성: 0.11349999904632568\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 146\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39m# 모델 크기 비교\u001b[39;00m\n\u001b[0;32m    145\u001b[0m original_model_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(loaded_model\u001b[39m.\u001b[39mto_json())\n\u001b[1;32m--> 146\u001b[0m pruned_model_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(pruned_keras_file\u001b[39m.\u001b[39;49mto_json())\n\u001b[0;32m    147\u001b[0m quantized_model_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(quantized_model\u001b[39m.\u001b[39mto_json())\n\u001b[0;32m    149\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m원본 모델 크기:\u001b[39m\u001b[39m\"\u001b[39m, original_model_size, \u001b[39m\"\u001b[39m\u001b[39mbytes\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'to_json'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import time\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# get_gzipped_model_size 함수 정의\n",
    "def get_gzipped_model_size(model):\n",
    "    _, keras_file = tempfile.mkstemp('.h5')\n",
    "    tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "    \n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(keras_file)\n",
    "    \n",
    "    return os.path.getsize(zipped_file)\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=1,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Save the model in HDF5 format\n",
    "model.save(\"my_model.h5\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model(\"my_model.h5\")\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "validation_split = 0.1  # 10% of training set will be used for the validation set.\n",
    "\n",
    "num_images = train_images.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                             final_sparsity=0.80,\n",
    "                                                             begin_step=0,\n",
    "                                                             end_step=end_step)\n",
    "}\n",
    "\n",
    "# Apply pruning to the loaded model\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "model_for_pruning = prune_low_magnitude(loaded_model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(train_images, train_labels,\n",
    "                      batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                      callbacks=callbacks)\n",
    "\n",
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)\n",
    "\n",
    "# 가지치기 모델의 가지치기 제거\n",
    "final_pruned_model = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "\n",
    "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(final_pruned_model, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)\n",
    "\n",
    "\n",
    "# 양자화 함수 정의\n",
    "def apply_quantization_to_loaded_model(loaded_model):\n",
    "    quantize_model = tfmot.quantization.keras.quantize_model(loaded_model)\n",
    "    quantize_model.compile(optimizer='adam',\n",
    "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                           metrics=['accuracy'])\n",
    "    return quantize_model\n",
    "\n",
    "# 양자화 모델 생성\n",
    "quantized_model = apply_quantization_to_loaded_model(final_pruned_model)\n",
    "\n",
    "# TFLite 변환 (가지치기 모델을 양자화 모델로 변환)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quantized_model)\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "# TFLite 모델 저장\n",
    "with open(\"quantized_model.tflite\", \"wb\") as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "\n",
    "# 양자화 모델 평가\n",
    "_, quantized_accuracy = quantized_model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"양자화 모델 정확성:\", quantized_accuracy)\n",
    "\n",
    "# 원본 모델 평가\n",
    "_, original_accuracy = loaded_model.evaluate(test_images, test_labels)\n",
    "print(\"원본 모델 정확성:\", original_accuracy)\n",
    "\n",
    "# 성능 비교\n",
    "print(\"성능 비교:\")\n",
    "print(\"가지치기 모델 정확성:\", model_for_pruning_accuracy)\n",
    "print(\"양자화 모델 정확성:\", quantized_accuracy)\n",
    "\n",
    "# 모델 크기 비교\n",
    "original_model_size = len(loaded_model.to_json())\n",
    "pruned_model_size = len(pruned_keras_file.to_json())\n",
    "quantized_model_size = len(quantized_model.to_json())\n",
    "\n",
    "print(\"원본 모델 크기:\", original_model_size, \"bytes\")\n",
    "print(\"가지치기 모델 크기:\", pruned_model_size, \"bytes\")\n",
    "print(\"양자화 모델 크기:\", quantized_model_size, \"bytes\")\n",
    "\n",
    "# 압축된 모델 크기 비교\n",
    "original_gzipped_model_size = get_gzipped_model_size(loaded_model)\n",
    "pruned_gzipped_model_size = get_gzipped_model_size(pruned_keras_file)\n",
    "quantized_gzipped_model_size = get_gzipped_model_size(quantized_model)\n",
    "\n",
    "print(\"원본 모델 압축 크기:\", original_gzipped_model_size, \"bytes\")\n",
    "print(\"가지치기 모델 압축 크기:\", pruned_gzipped_model_size, \"bytes\")\n",
    "print(\"양자화 모델 압축 크기:\", quantized_gzipped_model_size, \"bytes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

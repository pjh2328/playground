{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 및 모듈 설치\n",
    "import tempfile\n",
    "import os\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 크기 계산 함수\n",
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(test_images):\n",
    "    if i % 1000 == 0:\n",
    "      print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = (prediction_digits == test_labels).mean()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "#모델 로드\n",
    "model = tf.keras.models.load_model(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/313 [..............................] - ETA: 22s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "로드된 모델의 정확도: 0.9769\n"
     ]
    }
   ],
   "source": [
    "# 모델을 사용하여 테스트 데이터에 대한 예측 수행\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "# 예측 결과를 실제 레이블과 비교하여 정확도 계산\n",
    "accuracy = (predictions.argmax(axis=1) == test_labels).mean()\n",
    "print(\"로드된 모델의 정확도:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "422/422 [==============================] - 5s 9ms/step - loss: 0.0715 - accuracy: 0.9793 - val_loss: 0.0727 - val_accuracy: 0.9818\n",
      "Epoch 2/4\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0803 - accuracy: 0.9773 - val_loss: 0.0809 - val_accuracy: 0.9797\n",
      "Epoch 3/4\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0836 - accuracy: 0.9768 - val_loss: 0.0772 - val_accuracy: 0.9797\n",
      "Epoch 4/4\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0761 - accuracy: 0.9780 - val_loss: 0.0714 - val_accuracy: 0.9800\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_reshap  (None, 28, 28, 1)         1         \n",
      " e_1 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 26, 26, 12)        230       \n",
      " _1 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_max_po  (None, 13, 13, 12)        1         \n",
      " oling2d_1 (PruneLowMagnitu                                      \n",
      " de)                                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_flatte  (None, 2028)              1         \n",
      " n_1 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_  (None, 10)                40572     \n",
      " 1 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40805 (159.41 KB)\n",
      "Trainable params: 20410 (79.73 KB)\n",
      "Non-trainable params: 20395 (79.69 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 128\n",
    "epochs = 4\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = train_images.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "model_for_pruning.fit(train_images, train_labels,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)\n",
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "# 프루닝 모델 저장\n",
    "model_for_pruning.save(\"purned_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "108/108 [==============================] - 5s 38ms/step - loss: 0.0729 - accuracy: 0.9795 - val_loss: 0.0684 - val_accuracy: 0.9798\n",
      "Epoch 2/4\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 0.0656 - accuracy: 0.9810 - val_loss: 0.0669 - val_accuracy: 0.9805\n",
      "Epoch 3/4\n",
      "108/108 [==============================] - 4s 34ms/step - loss: 0.0615 - accuracy: 0.9823 - val_loss: 0.0641 - val_accuracy: 0.9820\n",
      "Epoch 4/4\n",
      "108/108 [==============================] - 4s 34ms/step - loss: 0.0590 - accuracy: 0.9829 - val_loss: 0.0636 - val_accuracy: 0.9818\n",
      "q test accuracy: 0.9793000221252441\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_2 (Quantize  (None, 28, 28)            3         \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " quant_reshape_1 (QuantizeW  (None, 28, 28, 1)         1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_conv2d_1 (QuantizeWr  (None, 26, 26, 12)        147       \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_1 (Qua  (None, 13, 13, 12)        1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_flatten_1 (QuantizeW  (None, 2028)              1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_1 (QuantizeWra  (None, 10)                20295     \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20448 (79.88 KB)\n",
      "Trainable params: 20410 (79.73 KB)\n",
      "Non-trainable params: 38 (152.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "q_aware_model.fit(train_images, train_labels,\n",
    "                  batch_size=500, epochs=4, validation_split=0.1)\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "print('q test accuracy:', q_aware_model_accuracy)\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_aware_model.save(\"quantized_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 크기를 정수로 변환하여 출력\n",
    "keras_file_size = int(get_gzipped_model_size(\"my_model.h5\"))\n",
    "pruned_keras_file_size = int(get_gzipped_model_size(\"purned_model.h5\"))\n",
    "quant_file_size = int(get_gzipped_model_size(\"quantized_model.h5\"))\n",
    "#압축률 = (압축 전 데이터 크기) / (압축 후 데이터 크기)\n",
    "pruned_keras_compression_rate = ((keras_file_size / pruned_keras_file_size))\n",
    "quant_file_compression_rate = (keras_file_size / quant_file_size)\n",
    "#원본 모델 비교\n",
    "PADP = (accuracy - (accuracy - model_for_pruning_accuracy)) / accuracy * 100\n",
    "PADQ = (accuracy - (accuracy - q_aware_model_accuracy)) / accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "모델 사이즈\n",
      "-------------------------------------------------------------\n",
      "Size of gzipped baseline Keras model: 234324 bytes\n",
      "Size of gzipped pruned Keras model: 186048 bytes\n",
      "Size of gzipped Quantized model: 151059 bytes\n",
      "-------------------------------------------------------------\n",
      "모델 압축률\n",
      "-------------------------------------------------------------\n",
      "compression rate of pruned Keras model:  1.2594814241486068\n",
      "compression rate of Quantized model:  1.5512084682144063\n",
      "-------------------------------------------------------------\n",
      "모델 최적화 성능\n",
      "-------------------------------------------------------------\n",
      "Baseline test accuracy: 0.9769\n",
      "Pruned test accuracy: 0.9764999747276306\n",
      "Quant TF test accuracy: 0.9793000221252441\n",
      "-------------------------------------------------------------\n",
      "모델 최적화 정확도 비교리포트\n",
      "-------------------------------------------------------------\n",
      "가지치기 비교 정확도 : 99.9590515638889\n",
      "양자화 비교 정확도 : 100.24567735952954\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------------------------------------------------------')\n",
    "print('모델 사이즈')\n",
    "print('-------------------------------------------------------------')\n",
    "print(\"Size of gzipped baseline Keras model: %d bytes\" % keras_file_size)\n",
    "print(\"Size of gzipped pruned Keras model: %d bytes\" % pruned_keras_file_size)\n",
    "print(\"Size of gzipped Quantized model: %d bytes\" % quant_file_size)\n",
    "print('-------------------------------------------------------------')\n",
    "print('모델 압축률')\n",
    "print('-------------------------------------------------------------')\n",
    "print(\"compression rate of pruned Keras model: \", pruned_keras_compression_rate)\n",
    "print(\"compression rate of Quantized model: \", quant_file_compression_rate)\n",
    "print('-------------------------------------------------------------')\n",
    "print('모델 최적화 성능')\n",
    "print('-------------------------------------------------------------')\n",
    "print('Baseline test accuracy:', accuracy)\n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)\n",
    "print('Quant TF test accuracy:', q_aware_model_accuracy)\n",
    "print('-------------------------------------------------------------')\n",
    "print('모델 최적화 정확도 비교리포트')\n",
    "print('-------------------------------------------------------------')\n",
    "print('가지치기 비교 정확도 :', PADP)\n",
    "print('양자화 비교 정확도 :', PADQ)\n",
    "print('-------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
